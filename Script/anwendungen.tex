\chapter{Anwendungen}
In diesem Kapitel stellen wir verschiedene Anwendungen der bisher entwickelten Theorie vor.
Zun\"achst zeigen wir, wie sich bestimmte transzendente Funktionen wie der nat\"urliche Logarithmus und die
trigonometrischen Funktionen effektiv mit Hilfe von Reihen berechnen lassen.   Anschlie{\ss}end diskutieren
wir, wann eine Funktion sich durch ein Polynom interpolieren l\"asst.
Danach besprechen wir das Newton'sche Verfahren zur Bestimmung von Nullstellen, welches dann anwendbar ist,
wenn die Funktion, deren Nullstelle bestimmt werden soll, differenzierbar ist.  Au{\ss}erdem untersuchen 
wir die Konvergenz von Fixpunkt-Verfahren und zeigen, wie sich lineare Gleichungs-Systeme mit Hilfe von
Fixpunkt-Verfahren approximativ l\"osen lassen.

\section{Taylor-Reihen}
Es sei $f:\mathbb{R} \rightarrow \mathbb{R}$ eine Funktion, die beliebig oft
differenzierbar ist. Wir stellen uns die Frage, ob es m\"oglich ist, eine solche Funktion
als Potenzreihe darzustellen, wir fragen also, ob es eine Folge $\folge{a_n}$ gibt, so
dass 
\begin{equation}
  \label{eq:taylor}
 f(x) = \sum\limits_{n=0}^\infty a_n \cdot  x^n  
\end{equation}
gilt.  Falls eine solche Folge $\folge{a_n}$ existiert, dann m\"ochten wir diese Folge
berechnen k\"onnen.  Wenn die Gleichung (\ref{eq:taylor}) g\"ultig ist, dann k\"onnen wir
den Koeffizienten $a_0$ dadurch berechnen, dass wir in dieser Gleichung $x=0$ setzen.
Wir erhalten dann
\begin{equation}
  \label{eq:taylor0}
 f(0) = a_0 + \sum\limits_{n=1}^\infty a_n \cdot  0^n  = a_0.
\end{equation}
Um den Koeffizienten $a_1$ zu berechnen, differenzieren wir Gleichung (\ref{eq:taylor}):

\begin{equation}
  \label{eq:taylor1}
 \df f(x) = a_1 \cdot  1 \cdot  x^0 + \sum\limits_{n=2}^\infty a_n \cdot  n \cdot  x^{n-1}.
\end{equation}
Setzen wir in dieser Gleichung $x=0$, so finden wir
\begin{equation}
  \label{eq:taylor2}
 \df f(0) = a_1 + \sum\limits_{n=2}^\infty a_n \cdot  n \cdot  0^{n-1} = a_1.
\end{equation}
Allgemein k\"onnen wir den Koeffizienten $a_k$ dadurch bestimmen, dass wir Gleichung
(\ref{eq:taylor}) $k$-mal nach $x$ differenzieren und anschlie{\ss}end $x = 0$ setzen.
Wir beweisen zun\"achst durch Induktion \"uber $k$, dass f\"ur alle $k\in\mathbb{N}_0$ 
\begin{equation}
  \label{eq:taylorDiff}
  \begin{array}[t]{lcl}    
  f^{(k)}(x) & = & \ds\sum\limits_{n=k}^\infty a_n \cdot  n \cdot  (n-1) \cdot  \cdots \cdot  \bigl(n-(k-1)\bigr) \cdot  x^{n-k} \\[0.3cm]
             & = & \ds\sum\limits_{n=k}^\infty 
                   a_n \cdot \left( \prod\limits_{i=0}^{k-1} (n-i)\right) \cdot x^{n-k}
                   \\[0.4cm]
             & = & \ds\sum\limits_{n=k}^\infty \bruch{n!}{(n-k)!} \cdot a_n \cdot x^{n-k}
  \end{array}
\end{equation}
gilt.  Hierbei bezeichnet $f^{(k)}(x)$ die $k$-te Ableitung der Funktion $f$ an der Stelle
$x$.
\begin{enumerate}
\item[I.A.:] $k = 0$. \quad Es gilt \\[0.3cm]
              \hspace*{1.3cm}
              $
              \begin{array}[t]{lcl}              
              f^{(0)}(x) & = & f(x) \\[0.3cm]
                         & = & \ds\sum\limits_{n=0}^\infty a_n \cdot x^{n} \\[0.5cm]
                         & = & \ds\sum\limits_{n=k}^\infty \bruch{n!}{(n-0)!} \cdot a_n \cdot x^{n-k}.
              \end{array}
              $ 
\item[I.S.:] $k \mapsto k + 1$.  \quad Es gilt 
             \\[0.3cm]
             \hspace*{1.3cm}
             $
             \begin{array}[t]{lcl}
             f^{(k+1)}(x) & = & \df f^{(k)}(x) \\[0.3cm]
             & \stackrel{IV}{=} & 
               \ds \dfo \sum\limits_{n=k}^\infty \bruch{n!}{(n-k)!} \cdot a_n \cdot x^{n-k} 
             \\[0.5cm]
             & = & \ds\sum\limits_{n=k+1}^\infty \bruch{n!}{(n-k)!} \cdot (n-k) \cdot a_n \cdot x^{n-k-1} 
                   \\[0.5cm]
             & = & \ds\sum\limits_{n=k+1}^\infty \bruch{n!}{(n-k-1)!} \cdot a_n \cdot x^{n-(k+1)} 
                   \\[0.5cm]
             & = & \ds\sum\limits_{n=k+1}^\infty \bruch{n!}{\bigl(n-(k+1)\bigr)!} \cdot a_n \cdot x^{n-(k+1)} 
             \end{array}
             $
\end{enumerate}
Damit ist der Beweis von Gleichung (\ref{eq:taylorDiff}) abgeschlossen.
Setzen wir in dieser Gleichung f\"ur $x$ den Wert $0$ ein, so erhalten wir 
\\[0.3cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
f^{(k)}(0) & = & \bruch{k!}{(k-k)!} \cdot a_k  + 
                 \sum\limits_{n=k+1}^\infty \bruch{n!}{(n-k)!} \cdot a_n \cdot 0^{n-k} 
                 \\[0.5cm]
           & = & k! \cdot a_k  
\end{array}
$
\\[0.3cm]
Dividieren wir diese Gleichung durch $k!$, so haben wir f\"ur die Koeffizienten der Taylor-Reihe die Formel 
\\
\hspace*{1.3cm}
$a_k = \bruch{f^{(k)}(0)}{k!}$
\\[0.3cm]
gefunden.  Also definieren wir f\"ur eine Funktion $f:\mathbb{R} \rightarrow \mathbb{R}$, die im Punkt $x=0$
beliebig oft differenzierbar ist, die der Funktion $f$ zugeordnete \emph{Taylor-Reihe} als
\begin{equation}
  \label{eq:taylorFormula}
  \colorbox{red}{\framebox{\colorbox{orange}{$\ds \textsl{taylor}(f,x) := \sum\limits_{n=0}^\infty \bruch{f^{(n)}(0)}{n!} \cdot  x^n$.}}}
\end{equation}


\remark
Im Allgemeinen wissen wir nicht, ob die Reihe $\textsl{taylor}(f,x)$ konvergiert.  Selbst
wenn die Reihe konvergiert folgt daraus noch nicht, dass $f(x) = \textsl{taylor}(f,x)$
ist.  Als Beispiel dazu betrachten wir die Funktion $f:\mathbb{R} \rightarrow \mathbb{R}$, die durch
\\[0.2cm]
\hspace*{1.3cm}
$f(x) := \left\{
\begin{array}{ll}
  \exp\Bigl(-\bruchs{1}{x^2}\Bigr)  & \mbox{falls $x \not= 0$} \\[0.3cm]
   0                              & \mbox{falls $x = 0$}
\end{array}
\right.
$
\\[0.2cm]
definiert ist.  Im Buch von Otto Forster \cite{forster:2011} wird gezeigt, dass f\"ur diese Funktion
die Werte s\"amtlicher Ableitungen an der Stelle $x = 0$ verschwinden.   Damit gilt dann
 $\textsl{taylor}(f,x) = 0$.  \eox


\subsection{Der Abbruch-Fehler bei der Taylor-Reihe}
Um zu untersuchen, wann die Taylor-Reihe $\textsl{taylor}(f,x)$ gegen $f(x)$
konvergiert, definieren wir zu einer gegebenen Funktion $f$ und einer nat\"urlichen Zahl
$n\in \mathbb{N}_0$ den \emph{Abbruch-Fehler vom Grad $n$} als
\\[0.3cm]
\hspace*{1.3cm}
$\textsl{error}_n(x) := f(x) - \displaystyle\sum\limits_{i=0}^n \bruch{f^{(i)}(0)}{i!} \cdot  x^i$.
\\[0.3cm]
Der Abbruch-Fehler gibt also an, wie gro{\ss} der Fehler ist, wenn wir die Berechnung der Summe einer
Taylor-Reihe nach dem $n$-ten Glied abbrechen.
Wir berechnen  eine Absch\"atzung f\"ur den Abbruch-Fehler
$\textsl{error}_n(x)$.  Dazu benutzen wir den erweiterten Mittelwert-Satz.
Zun\"achst bemerken wir, dass f\"ur alle $k=0,\cdots,n$ die $k$-te Ableitung
des Abbruch-Fehlers vom Grad $n$ den Wert 0 hat:
\\[0.3cm]
\hspace*{1.3cm}
$\textsl{error}_n^{(k)}(0) = 0$
\\[0.3cm]
Dies folgt aus der Definition des Abbruch-Fehlers, denn wir hatten die Taylor-Reihe ja
gerade so definiert, dass Sie mit der Funktion $f$ an der Stelle $0$ in allen Ableitungen
\"ubereinstimmt.   
Jetzt wenden wir auf die Funktionen $\textsl{error}_n(x)$ und $g_0(x) := x^{n+1}$ in dem Intervall
$[0,x]$ den erweiterten 
Mittelwert-Satz an.  Dann gibt es ein $\chi_1 \in [0,x]$, so dass 
\begin{equation}
  \label{eq:taylorErr0}  
\bruch{\dfo \err{\chi_1}}{ \dfo g_0(\chi_1)} = \bruch{\textsl{error}_n(x) - \textsl{error}_n(0)}{g_0(x) - g_0(0)}
\end{equation}
gilt. 
F\"ur die Ableitung der Funktion $g_0(x) = x^{n+1}$ finden wir $\dfo g_0(x) = (n+1) \cdot  x^n$.
Wegen $\err{0} = 0$ und $g_0(0) = 0$  vereinfacht sich Gleichung (\ref{eq:taylorErr0}) zu
\begin{equation}
  \label{eq:taylorErr0a}
   \bruch{\erri{\chi_1}{1}}{(n+1)\cdot \chi_1^n} = \bruch{\textsl{error}_n(x)}{x^{n+1}}.  
\end{equation}
Nun wenden wir in dem Interval $[0,\chi_1]$ den erweiterten  Mittelwert-Satz 
auf die beiden Funktionen $\erri{x}{1}$ und 
$g_1(x) := (n+1)\cdot x^n$ an.  Dann gibt es ein $\chi_2 \in[0,\chi_1]$, so dass
\begin{equation}
  \label{eq:taylorErr1}  
\bruch{\dfo \erri{\chi_2}{1}}{ \dfo g_1(\chi_2)} = \bruch{\erri{\chi_1}{1} - \erri{0}{1}}{g_1(\chi_1) - g_1(0)}
\end{equation}
gilt.
F\"ur die Ableitung der Funktion $g_1(x) = (n+1)\cdot x^{n}$ finden wir $\dfo g_1(x) = (n+1) \cdot  n \cdot x^{n-1}$.
Wegen $\erri{0}{1} = 0$ und $g_1(0) = 0$  vereinfacht sich Gleichung (\ref{eq:taylorErr1})
unter Ber\"ucksichtigung von Gleichung (\ref{eq:taylorErr0a}) zu
\\[0.3cm]
\hspace*{1.3cm} $\bruch{\erri{\chi_2}{2}}{(n+1)\cdot n\cdot \chi_2^{n-1}} =
\bruch{\erri{\chi_1}{1}}{(n+1)\cdot \chi_1^{n}} = \bruch{\textsl{error}_n(x)}{x^{n+1}}$.
\\[0.3cm]
Dieses Spiel k\"onnen wir fortsetzen.  Wenn wir $k$-mal den erweiterten Mittelwert-Satz
anwenden und $k \leq n$ ist, erhalten wir ein $\chi_k \in [0,\chi_{k-1}]$, so dass gilt:
\begin{equation}
  \label{eq:taylorErrk}
\bruch{\erri{\chi_k}{k}}{\frac{(n+1)!}{(n+1-k)!}\cdot \chi_k^{n+1-k}} = 
  \bruch{\textsl{error}_n(x)}{x^{n+1}}  
\end{equation}
Um diese Behauptung per Induktion nach $k$ zu beweisen, bemerken wir, dass der Induktions-Anfang
$k=1$ bereits bewiesen wurde.  Im Induktions-Schritt
wenden wir in dem Interval $[0,\chi_k]$
auf die beiden Funktionen $\erri{\chi_k}{k}$ und 
$g_{k}(x) :=  \bruch{(n+1)!}{(n+1 - k)!}\cdot x^{n+1-k}$ den erweiterten
Mittelwert-Satz an.  Wir finden dann ein $\chi_{k+1} \in [0,\chi_k]$, so dass
\begin{equation}
  \label{eq:taylorDiffInd}
\bruch{\dfo \erri{\chi_{k+1}}{k}}{\dfo g_{k}(\chi_{k+1})} =
 \bruch{\erri{\chi_k}{k} - \erri{0}{k}}{g_{k}(\chi_k) - g_{k}(0)} 
\end{equation}
gilt.  
F\"ur die Ableitung der Funktion $g_k(x)$ finden wir 
\\[0.3cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
  \dfo g_k(x) & = & \bruch{(n+1)!}{(n+1-k)!} \cdot (n+1-k) \cdot x^{n+1-k-1} \\[0.3cm]
             & = &  \bruch{(n+1)!}{(n+1-(k+1))!} \cdot x^{n+1-(k+1)} \\[0.3cm]
             & = & g_{k+1}(x)
\end{array}
$
\\[0.3cm]
Wegen $\erri{0}{k} = 0$ und $g_k(0) = 0$  vereinfacht sich Gleichung
(\ref{eq:taylorDiffInd}) zu \\[0.3cm]
\hspace*{1.3cm}
$\bruch{\erri{\chi_{k+1}}{k+1}}{g_{k+1}(\chi_{k+1})} =
 \bruch{\erri{\chi_k}{k}}{g_{k+1}(\chi_k)} 
$
\\[0.3cm]
Ber\"ucksichtigen wir hier noch die Induktions-Voraussetzung (\ref{eq:taylorErrk}),
so haben wir
\begin{equation}
  \label{eq:taylorErrk1}
  \bruch{\erri{\chi_{k+1}}{k+1}}{g_{k+1}(\chi_{k+1})} = \bruch{\textsl{error}_n(x)}{x^{n+1}}
\end{equation}
gefunden und dadurch die Formel (\ref{eq:taylorErrk}) per Induktion nachgewiesen.
Setzen wir in der Gleichung
(\ref{eq:taylorErrk}) f\"ur $k$ den Wert $n$ ein, so haben wir
\begin{equation}
  \label{eq:taylorErrna}
\bruch{\erri{\chi_n}{n}}{(n+1)!\cdot \chi_n} = \bruch{\textsl{error}_n(x)}{x^{n+1}}
\end{equation}
gezeigt. Wir wenden den erweiterten Mittelwert-Satz auf die Funktionen $\erri{x}{n}$
und $x \mapsto (n+1)!\cdot x$ an.  Dann erhalten wir ein $\chi \in [0,\chi_n] \subseteq [0,x]$, so dass
\begin{equation}
  \label{eq:taylorErrn}
  \bruch{\dfo \erri{\chi}{n}}{\dfo (n+1)!\cdot x (\chi)} = 
  \bruch{\erri{\chi_{n}}{n} - \erri{0}{n}}{(n+1)!\cdot \chi_n - (n+1)!\cdot 0}
\end{equation}
gilt.  Wegen $\erri{0}{n} = 0$ haben wir also
\begin{equation}
  \label{eq:taylorErrn1}
  \bruch{\erri{\chi}{n+1}}{(n+1)!} = \bruch{\erri{\chi_{n}}{n}}{(n+1)!\cdot \chi_n}. 
\end{equation}
Um diese Gleichung zu vereinfachen, errinnern wir daran, dass 
$\err{x}$ als 
\\[0.2cm]
\hspace*{1.3cm}
$\err{x} = f(x) - \displaystyle\sum\limits_{k=0}^n \bruch{f^{(k)}(0)}{k!} \cdot  x^k$
\\[0.2cm]
definiert ist.  Wenn wir die $(n+1)$-te Ableitung der Funktion $\err{x}$ bilden, dann bleibt
von der Summe nichts \"uber, es gilt also 
\\[0.2cm]
\hspace*{1.3cm} $\erri{x}{n+1} = f^{(n+1)}(x)$.
\\[0.2cm]
Setzen wir dieses Ergebnis in Gleichung (\ref{eq:taylorErrn1}) ein und ber\"ucksichtigen
Gleichung (\ref{eq:taylorErrna}), so finden wir
\begin{equation}
  \label{eq:taylorErrnn}
    \bruch{f^{(n+1)}(\chi)}{(n+1)!} = \bruch{\textsl{error}_n(x)}{x^{n+1}}. 
\end{equation}
Setzen wir hier die Definition von $\textsl{error}_n(x)$ ein und multiplizieren die Gleichung mit
$x^{n+1}$, so haben wir gezeigt, dass es ein $\chi \in [0,x]$ gibt, so dass
\begin{equation}
  \label{eq:taylorLagrange}
  \colorbox{red}{\framebox{\colorbox{orange}{$\ds f(x) = \sum\limits_{k=0}^n \bruch{f^{(k)}(0)}{k!} \cdot  x^k + f^{(n+1)}(\chi)\cdot \frac{x^{n+1}}{(n+1)!}$}}}
\end{equation}
gilt.  Diese Formel bezeichnen wir als die 
\emph{Taylor-Entwicklung} 
(\href{http://en.wikipedia.org/wiki/Brook_Taylor}{Brook Taylor}, 1685 -- 1731) der Funktion $f$ mit \emph{Lagrange'schem Restglied}
(\href{http://en.wikipedia.org/wiki/Lagrange}{Joseph Louis Lagrange}, 1736 -- 1813). 

\section{Beispiele von Taylor-Entwicklungen}
Wir zeigen nun, wie wir transzendente Funktionen 
mit Hilfe der Taylor-Entwicklungen approximieren k\"onnen.  Dadurch werden diese Funktionen
einer numerischen Behandlung zug\"anglich.  

\subsection{Berechnung des nat\"urlichen Logarithmus}
Wir beginnen mit dem nat\"urlichen Logarithmus $x \mapsto \ln(x)$.  Dieser ist als die Umkehrfunktion der
Exponential-Funktion definiert, es gilt also
\\[0.2cm]
\hspace*{1.3cm}
$\ln\bigl(\exp(x)\bigr) = x$.
\\[0.2cm]
Da die Exponential-Funktion immer positiv ist, ist der nat\"urliche Logarithmus f\"ur $x \leq 0$  nicht
definiert.  Damit macht eine Taylor-Entwicklung der Funktion $f(x) := \ln(x)$ keinen Sinn, denn in
der Taylor-Reihe ist schon der konstante Koeffizient 
\\[0.2cm]
\hspace*{1.3cm}
$\ds\frac{f^{(0)}(0)}{0!} = f(0)$
\\[0.2cm]
undefiniert.  Alle weiteren Koeffizienten sind nat\"urlich ebenfalls undefiniert.
Wir betrachten daher die Funktion $f(x)  := \ln(1 + x)$, denn diese Funktion ist f\"ur $x = 0$
definiert.  Zun\"achst berechnen wir die 
Ableitungen dieser Funktion.  Wir beweisen durch Induktion, dass f\"ur alle
$n \in \mathbb{N}$ die $n$-te Ableitung der Funktion $f$ die folgende Form hat:
\\[0.2cm]
\hspace*{1.3cm} $f^{(n)}(x) = (-1)^{n+1}\cdot \bruch{(n-1)!}{(1+x)^n}$
\begin{enumerate}
\item[I.A.:] $n=1$.  Es gilt
  \\[0.2cm]
  \hspace*{1.3cm}
  $\dfo f(x) = \dfo \ln(1+x) = \bruch{1}{1+x} = (-1)^{1+1}\cdot \bruch{(1-1)!}{(1+x)^1}$,
  \\[0.2cm]
  denn $n!$ ist f\"ur $n\in\mathbb{N}_0$ so definiert, dass $0! = 1$ gilt.
\pagebreak
\item[I.S.:] $n \mapsto n+1$.  Wir haben 
  \\[0.3cm]
  \hspace*{1.3cm}
  $
  \begin{array}[t]{lcl}  
    f^{(n+1)}(x) & = & \dfo f^{(n)}(x) \\[0.3cm]
    & \stackrel{\mathrm{IV}}{=} & \dfo \left((-1)^{n+1}\cdot \bruch{(n-1)!}{(1+x)^n}\right) \\[0.5cm]
    & = & (-1)^{n+1}\cdot (n-1)!\cdot \bruch{(-n)}{(1+x)^{n+1}} \\[0.5cm]
    & = & (-1)^{(n+1)+1}\cdot \bruch{n!}{(1+x)^{n+1}} \\[0.3cm]
  \end{array}
  $  
\end{enumerate}
Setzen wir hier $x = 0$, so sehen wir, dass
\\[0.2cm]
\hspace*{1.3cm}
$f^{(n)}(0) = (-1)^{n+1}\cdot \bruch{(n-1)!}{(1+0)^n} = (-1)^{n+1}\cdot (n-1)!$ 
\\[0.3cm]
gilt.  Wegen $f(0) = \ln(1) = 0$ erhalten wir f\"ur die Taylor-Entwicklungen der Funktion $\ln(1+x)$ das Ergebnis 
\begin{equation}
  \label{eq:taylorLnSimple}
  \textsl{taylor}\bigl(x \mapsto \ln(1+x),x \bigr) = \sum\limits_{n=1}^\infty (-1)^{n+1}\cdot \bruch{(n-1)!\cdot x^n}{n!} = \sum\limits_{n=1}^\infty (-1)^{n+1}\cdot \bruch{x^n}{n}.
\end{equation}
Wir wollen nun zeigen, dass diese Taylor-Reihe tats\"achlich gegen $\ln(1+x)$ konvergiert, wir wollen also
zeigen, dass
\\[0.2cm]
\hspace*{1.3cm}
$\textsl{taylor}\bigl(x \mapsto \ln(1+x), x \bigr) = \ln(1+x)$
\\[0.2cm]
gilt.  Dazu betrachten
wir die Taylor-Entwicklung mit dem Lagrange'schen Restglied: 
\begin{equation}
  \label{eq:taylorLnLagrange}
  \ln(1+x) = \sum\limits_{k=1}^n  (-1)^{k+1}\cdot \bruch{x^k}{k} + (-1)^{n}\cdot \bruch{1}{(1+\chi)^{n+1}}\cdot \bruch{x^{n+1}}{n+1}
\end{equation}
F\"ur den Abbruch-Fehler haben wir also
\\[0.2cm]
\hspace*{1.3cm} $\textsl{error}_n(x) = (-1)^n \cdot \bruch{1}{(1+\chi)^{n+1}}\cdot \bruch{x^{n+1}}{n+1}$
\\[0.2cm]
mit $\chi \in[0,x]$ und f\"ur $x \in [0, 1]$ geht dieser Wert f\"ur $n \rightarrow \infty$ gegen $0$.
Damit haben wir insgesamt $\textsl{taylor}\bigl(x \mapsto \ln(1+x), x \bigr) = \ln(1+x)$ f\"ur $x \in [0, 1]$ gezeigt und folglich k\"onnen
wir
\\[0.2cm]
\hspace*{1.3cm}
\colorbox{red}{\framebox{\colorbox{orange}{
$\ds\ln(1+x) = \sum\limits_{k=1}^{\infty} (-1)^{k+1}\cdot \frac{x^k}{k}$ \quad f\"ur $x \in (-1, 1]$}}}
\\[0.2cm]
schreiben.\footnote{
  Die Formel gilt auch f\"ur f\"ur negative $x$, deren Betrag kleiner als 1 ist, aber das 
  ist etwas schwieriger zu beweisen.
}
 Setzen wir hier f\"ur $x$ den Wert 1 ein, so haben wir die Formel
\\[0.2cm]
\hspace*{1.3cm}
$
\ln(2) = \displaystyle\sum\limits_{k=1}^\infty \bruch{(-1)^{k+1}}{k} 
       = 1 - \bruch{1}{2} + \bruch{1}{3} - \bruch{1}{4} + \bruch{1}{5} \pm \cdots
$
\\[0.2cm] 
gefunden.  Um den Abbruch-Fehler abzusch\"atzen, setzen wir in $\textsl{error}_n(x)$ f\"ur $x$ den Wert 1 ein
und finden
\\[0.2cm]
\hspace*{1.3cm}
$|\textsl{error}_n(1)| \leq \bruch{1}{n+1}$.
\\[0.2cm]
Um $\ln(2)$ also nach der
obigen Formel auf eine Genauigkeit von $10^{-9}$ berechnen zu k\"onnen, m\"ussten wir
$1\,000\,000\,000$ Terme aufsummieren!  Erfreulicherweise geht es auch effizienter.  Dazu ersetzen wir in
Gleichung (\ref{eq:taylorLnSimple}) $x$ durch $-x$ und erhalten
\begin{equation}
  \label{eq:taylorLnSimpleMinus}
  \ln(1-x) = \sum\limits_{k=1}^\infty (-1)^{k+1}\cdot \bruch{(-x)^k}{k} 
    = \sum\limits_{k=1}^\infty (-1)^{k+1}\cdot \bruch{(-1)^k\cdot x^k}{k} 
    = -\sum\limits_{k=1}^n \bruch{x^k}{k} 
\end{equation}
Subtrahieren wir diese Gleichung von der  Gleichung (\ref{eq:taylorLnLagrange}), so erhalten
wir 
\begin{equation}
  \label{eq:taylorLnEfficient}
  \begin{array}[b]{lcl}
   \ln\Bigl(\bruch{1+x}{1-x}\Bigr) & = & \ln(1+x) - \ln(1-x)  \\[0.3cm]
   & = & \displaystyle\sum\limits_{k=1}^\infty (-1)^{k+1}\cdot  \bruch{x^k}{k} + \sum\limits_{k=1}^n \bruch{x^k}{k} \\[0.5cm]
   & = & \displaystyle\sum\limits_{k=1}^\infty \bigl((-1)^{k+1} + 1\bigr) \cdot  \bruch{x^k}{k} \\[0.5cm]
   & = & 2 \cdot  \displaystyle\sum\limits_{n=0}^\infty \bruch{x^{2\cdot n+1}}{2\cdot n+1}, \\[0.5cm]
  \end{array}
\end{equation}
denn $(-1)^k+1 + 1$ hat f\"ur gerade Werte von $k$ den Wert $0$ und f\"ur ungerade Werte hat dieser
Term den Wert 2.
Setzen wir hier f\"ur $x$ den Wert $\ds\frac{1}{3}$ ein, so erhalten wir 
\\[0.2cm]
\hspace*{1.3cm}
$\displaystyle \ln\left(\frac{1+\frac{1}{3}}{1-\frac{1}{3}}\right) =  
 \ln\left(\frac{\;\frac{4}{3}\;}{\frac{2}{3}}\right) = \ln(2) = 
 2\cdot \sum\limits_{n=0}^\infty \frac{1}{2\cdot n+1} \cdot  \left(\frac{1}{3}\right)^{2\cdot n+1}
$
\\[0.2cm]
Um den Fehler $e$ abzusch\"atzen, den wir erhalten, wenn wir diese Reihe nach dem
Glied $2\cdot n+1$ abbrechen, sch\"atzen wir den Abbruch-Fehler wie folgt ab:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{clcl}
        & \multicolumn{3}{l}{\ds 
            \left| \ln\left(\frac{1+\frac{1}{3}}{1-\frac{1}{3}}\right) - 2\cdot \sum\limits_{k=0}^n \frac{1}{2\cdot k+1} \cdot  \left(\frac{1}{3}\right)^{2\cdot k+1} \right|} \\[0.5cm]
      = &\multicolumn{3}{l}{\ds 2 \cdot  \left| \sum\limits_{k=n+1}^\infty \frac{1}{2\cdot k+1} \cdot  \left(\frac{1}{3}\right)^{2\cdot k+1} \right|} \\[0.5cm]
   \leq &\ds  2 \cdot  \sum\limits_{k=n+1}^\infty \left(\frac{1}{3}\right)^{2\cdot k+1} 
     & = &\ds  2 \cdot  \sum\limits_{k=0}^\infty \left(\frac{1}{3}\right)^{2\cdot n+2\cdot k+3}  \\[0.5cm]
      = &\ds  2 \cdot  \left(\frac{1}{3}\right)^{2\cdot n+3} \sum\limits_{k=0}^\infty \left(\frac{1}{3}\right)^{2\cdot k}  
      & = &\ds  2 \cdot  \left(\frac{1}{3}\right)^{2\cdot n+3} \sum\limits_{k=0}^\infty \left(\frac{1}{9}\right)^{k}  \\[0.5cm]
       = &\ds  2 \cdot  \left(\frac{1}{3}\right)^{2\cdot n+3} \bruch{1}{1-\frac{1}{9}} 
     & = &\ds  2 \cdot  \left(\frac{1}{3}\right)^{2\cdot n+3} \bruch{9}{8}  \\[0.5cm]
      = &\ds  \frac{1}{4} \cdot  \left(\frac{1}{3}\right)^{2\cdot n+1}   \\[0.5cm]
\end{array}
$
\\[0.2cm]
Wir wollen $\ln(2)$ auf eine Genauigkeit von $10^{-9}$ berechnen.  Also w\"ahlen wir $n$ so,
dass gilt: 
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lrcl}
                & \ds\frac{1}{4} \cdot  \left(\frac{1}{3}\right)^{2\cdot n+1} & \leq 10^{-9} \\[0.3cm]
\Leftrightarrow & \ds\left(\frac{1}{3}\right)^{2\cdot n+1} & \leq 4 \cdot  10^{-9} \\[0.3cm]
\Leftrightarrow & \ds-\ln(3) \cdot  (2\cdot n+1)  & \leq \ln(4) - 9 \cdot  \ln(10) \\[0.3cm]
\Leftrightarrow & \ds          (2\cdot n+1)  & \geq \bruch{9 \cdot  \ln(10) - \ln(4)}{\ln(3)} \\[0.5cm]
\Leftrightarrow &           n        & \ds \geq 0.5\cdot \left(\bruch{9 \cdot  \ln(10) - \ln(4)}{\ln(3)}  - 1\right) \approx 8.3 \\[0.5cm]
\Leftarrow      &           n        & \geq 9 \\[0.3cm]
\end{array}
$
\\[0.2cm]
 Um $\ln(2)$ auf eine Genauigkeit von $10^{-9}$ zu berechnen reicht es also aus, wenn wir
 in der Formel (\ref{eq:taylorLnEfficient}) die ersten 9 Glieder der Summe ber\"ucksichtigen.  Wir erhalten
 \\[0.2cm]
 \hspace*{1.3cm} $\ln(2) \approx 2 \cdot  \displaystyle\sum\limits_{n=0}^9
 \bruch{1}{2\cdot n+1}\left(\frac{1}{3}\right)^{2\cdot n+1} \approx 0.69314718054981171974$
\\[0.2cm]
Der wirkliche Fehler ist sogar noch kleiner, er betr\"agt etwa $10^{-11}$.  Das liegt daran,
dass wir bei der Absch\"atzung der Summe durch die geometrische Reihe den Faktor $\ds\frac{1}{2\cdot
  k+1}$ vernachl\"assigt haben.
\vspace*{0.3cm}

Das Verfahren, das wir oben benutzt haben um $\ln(2)$ zu berechnen, l\"asst sich
verallgemeinern.  Ist die Aufgabe gegeben, f\"ur eine gegebene reelle Zahl $r$ den
nat\"urlichen Logarithmus $\ln(r)$ zu berechnen, so setzen wir 
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{clcl}
                & r = \bruch{1 + x}{1 - x} \\[0.3cm]
\Leftrightarrow & (1-x) \cdot  r = 1 + x        \\[0.3cm]
\Leftrightarrow & r - x \cdot  r = 1 + x        \\[0.3cm]
\Leftrightarrow & r - 1 = x + x \cdot  r        \\[0.3cm]
\Leftrightarrow & r - 1 = x \cdot  (1 + r)      \\[0.3cm]
\Leftrightarrow & \bruch{r - 1}{r+1} = x   \\[0.3cm]
\end{array}
$
\\[0.2cm]
Bei gegebenem $r$ bestimmen wir also $x$ nach der Formel $x=\ds\frac{r-1}{r+1}$.  F\"ur
das so bestimmte $x$ gilt dann
\begin{equation}
  \label{eq:Ln}
  \ln(r) = \ln\Bigl(\bruch{1+x}{1-x}\Bigr) = 2 \cdot  \displaystyle\sum\limits_{n=0}^\infty  \bruch{x^{2\cdot n+1}}{2\cdot n+1}  
  \quad \mbox{mit}\; x = \bruch{r-1}{r+1}.
\end{equation}

\exercise
Zeigen Sie, dass 
\\[0.2cm]
\hspace*{1.3cm}
$\ds 0 \leq \frac{r - 1}{r + 1} \leq \frac{1}{3}$
\\[0.2cm]
gilt, falls $r \in [1, 2]$ ist. \eox
\vspace*{0.3cm}

Die letzte Aufgabe zeigt, dass aus $r \in [1,2]$ die Ungleichung $0 \leq x \leq \frac{1}{3}$ folgt
und dann konvergiert die in Gleichung (\ref{eq:Ln}) gegebene Reihe sehr 
gut.  In modernen Rechnern werden reelle Zahlen $y$ in der Form
\\[0.2cm]
\hspace*{1.3cm}
$y = s \cdot  r \cdot  2^n$ \quad mit $s\in\{-1,+1\}$, \quad $r \in [1,2)$ \quad und $n\in\mathbb{Z}$
\\[0.2cm]
dargestellt.  Ist $y$ positiv, so l\"asst sich der nat\"urliche Logarithmus nach der Formel 
\\[0.2cm]
\hspace*{1.3cm}
$\ln(y) = \ln(r) + n \cdot  \ln(2)$
\\[0.2cm]
berechnen, wobei $\ln(r)$ mit Hilfe der Formel (\ref{eq:Ln}) gefunden wird.

\exercise
 Berechnen Sie die Taylor-Reihen f\"ur die Funktionen $x \mapsto \sin(x)$ und $x \mapsto \cos(x)$
 und geben Sie eine Absch\"atzung f\"ur den Abbruch-Fehler an.  Folgern Sie au{\ss}erdem die
\href{http://de.wikipedia.org/wiki/Eulersche_Formel}{Eulersche Formel}
\\[0.2cm]
\hspace*{1.3cm}
\colorbox{red}{\framebox{\colorbox{orange}{$\ds \mathrm{e}^{i \cdot x} = \cos(x) + i \cdot \sin(x)$,}}}
\\[0.2cm]
die auf 
\href{http://en.wikipedia.org/wiki/Leonard_Euler}{Leonard Euler} (1707 --- 1783) zur\"uck geht.
\eox


\subsection{Berechnung des Arcus-Tangens}
Die direkte Berechnung der Taylor-Reihe einer Funktion mit Hilfe der Formel
(\ref{eq:taylorFormula}) ist unter Umst\"anden sehr m\"uhsam.  Wollen wir beispielsweise die
Funktion $x \mapsto \arctan(x)$ in einer Taylor-Reihe entwickeln, so berechnen wir mit WolframAlpha$^{\scriptsize\textregistered}$ die
ersten f\"unf Ableitungen wie folgt:
\begin{enumerate}
\item $\arctan^{(1)}(x) = \bruch{1}{1+{x}^{2}}$.
\item $\arctan^{(2)}(x) = \displaystyle -2 \cdot {\frac {x}{ \left( 1+{x}^{2} \right) ^{2}}}$.
\item $\arctan^{(3)}(x) = \displaystyle 2\cdot {\frac {3\cdot{x}^{2}-1}{ \left( 1+{x}^{2} \right)^{3}}}$.
\item $\arctan^{(4)}(x) = \displaystyle -24\cdot{\frac {x\cdot \left({x}^{2} - 1\right) }{ \left( 1+{x}^{2}
      \right)^{4}}}$.
\item $\arctan^{(5)}(x) = \displaystyle 24 \cdot{\frac {1+5\cdot{x}^{4}-10\cdot{x}^{2}}{ \left( 1+{x}^{2} \right)^{5}}}$.
\end{enumerate}
Offenbar ist es schwierig, hier eine Struktur zu erkennen.

\exercise
Versuchen Sie, eine allgemeine Formel f\"ur die $n$-te Ableitung der Funktion \\
$x \mapsto \arctan(x)$ 
zu finden.  Beweisen Sie die Richtigkeit Ihrer Formel. 
\vspace*{0.2cm}

\noindent
\textbf{Hinweis:}  Es gilt
\\[0.2cm]
\hspace*{1.3cm}
$\ds\frac{1}{1 + x^2} = \frac{1}{2} \cdot \left(\frac{1}{1 + i \cdot x} + \frac{1}{1 - i \cdot x}\right)$.
\eox
\vspace*{0.2cm}

Wir gehen in der Vorlesung einen anderen Weg um die Taylor-Reihe der Arkustangens-Funktion
zu berechnen.  Dazu  stellen wir
die Ableitung der Funktion $x \mapsto\arctan(x)$ durch eine geometrische Reihe dar: 
\\[0.3cm]
\hspace*{1.3cm}
$\bruch{d\;}{dx}\arctan(x) = \bruch{1}{1+{x}^{2}} = \displaystyle \sum\limits_{n=0}^\infty \Bigl(-x^2\Bigr)^n = \sum\limits_{n=0}^\infty (-1)^n \cdot  x^{2\cdot n}$.
\\[0.3cm] 
Die Ableitung der Taylor-Reihe muss diese Reihe ergeben und au{\ss}erdem muss die Reihe an der
Stelle $0$ den Wert $0$ haben, denn es gilt $\arctan(0) = 0$.  Damit finden wir 
\begin{equation}
  \label{eq:taylorArctan}
  \arctan(x) = \sum\limits_{n=0}^\infty \bigl(-1\bigr)^n \cdot  \bruch{x^{2\cdot n+1}}{2\cdot n+1}
\end{equation}
Da $\ds\tan\Bigl(\frac{\pi}{4}\Bigr) = 1$, also $\ds\arctan\bigl(1) = \frac{\pi}{4}$ ist, haben wir die Formel
\begin{equation}
  \label{eq:PiViertel}
  \bruch{\pi}{4} = \sum\limits_{n=0}^\infty (-1)^n \cdot  \bruch{1}{2\cdot n+1} = 
  1 - \bruch{1}{3} + \bruch{1}{5} - \bruch{1}{7} + \bruch{1}{9} \pm \cdots
\end{equation}
gefunden.  F\"ur einen vollst\"andigen Beweis dieser Formel m\"ussten wir den Abbruch-Fehler
nach der Lagrange'schen Formel berechnen.  Das w\"urde uns jetzt allerdings zuviel Zeit
kosten.

\subsection{Berechnung von $\pi^*$}
Zur effizienten Berechnung von $\pi$ ist die  Formel (\ref{eq:PiViertel}) nicht geeignet.
Aus dem Beweis des Kriteriums von Leibniz f\"ur die Konvergenz alternierender Summen folgt,
dass der Abbruch-Fehler durch das erste weggelassene Glied abgesch\"atzt werden kann.  F\"ur die obige
Formel hei{\ss}t das, dass der Abbruch-Fehler wie folgt abgesch\"atzt werden kann:
\\[0.3cm]
\hspace*{1.3cm}
$\left|\arctan(x) - \sum\limits_{k=0}^n (-1)^k \cdot  \bruch{1}{2\cdot k+1}\right| \leq \bruch{1}{2\cdot (n+1)+1}$
\\[0.3cm]
\"Uberlegen wir, wieviele Glieder der Summe ben\"otigt werden, um $\ds\frac{\pi}{4}$ auf eine Genauigkeit von $10^{-9}$ 
zu berechnen.  Dann muss $n$ die folgende Ungleichung erf\"ullen:
\\[0.2cm]
\hspace*{1.3cm} 
$
\begin{array}[t]{lrcl} 
                & \bruch{1}{2\cdot n+3} & \leq & 10^{-9} \\[0.3cm]
\Leftrightarrow & 2\cdot n+3 \geq 10^9                   \\[0.2cm]
\Leftrightarrow & n \geq 0.5\cdot (10^9-3)                    \\[0.2cm]
\Leftrightarrow & n \geq 499\,999\,998.5                    \\[0.2cm]
\Leftarrow      & n \geq 499\,999\,999                    \\[0.2cm]
\end{array}
$
\\[0.2cm]
Wir  m\"ussten wir also etwa 500 Millionen Terme aufsummieren um die geforderte Genauigkeit
zu erreichen.  Um eine Formel zu erhalten, die schneller konvergiert, gehen wir von den
Additions-Theoremen von Sinus und Cosinus aus.  Diese lauten:
\\[0.2cm]
\hspace*{1.3cm}
$\sin(\alpha + \beta) = \sin(\alpha) \cdot \cos(\beta) + \cos(\alpha) \cdot \sin(\beta)$ \quad und
\\[0.2cm]
\hspace*{1.3cm}
$\cos(\alpha + \beta) = \cos(\alpha) \cdot \cos(\beta) - \sin(\alpha) \cdot \sin(\beta)$.
\\[0.2cm]
Teilen wir die erste Gleichung durch die zweite Gleichung, so folgt
\\[0.2cm]
\hspace*{1.3cm}
$\bruch{\sin(\alpha + \beta)}{\cos(\alpha + \beta)} = 
\bruch{\sin(\alpha) \cdot \cos(\beta) + \cos(\alpha) \cdot \sin(\beta)}{\cos(\alpha) \cdot \cos(\beta) - \sin(\alpha) \cdot \sin(\beta)}$.
\\[0.2cm]
Da $\tan(x) = \bruch{\sin(x)}{\cos(x)}$ ist, k\"onnen wir die linke Seite dieser Gleichung durch
$\tan(\alpha + \beta)$ ersetzen.  Auf der rechten Seite der Gleichung k\"urzen wir durch 
$\cos(\alpha) \cdot \cos(\beta)$.  Dann erhalten wir 
\\[0.2cm]
\hspace*{1.3cm}
$\ds\tan(\alpha + \beta) = 
\frac{\frac{\sin(\alpha)}{\cos(\alpha)} + \frac{\sin(\beta)}{\cos(\beta)}}{
       1 - \frac{\sin(\alpha)}{\cos(\alpha)} \cdot \frac{\sin(\beta)}{\cos(\beta)}}$.
\\[0.2cm] 
Ersetzen wir hier noch die Br\"uche der Form $\ds\frac{\sin(x)}{\cos(x)}$ durch $\tan(x)$, so haben wir
das Additions-Theorem f\"ur den Tangens gefunden:
\begin{equation}
  \label{eq:tangensAddition}
  \tan(\alpha + \beta) = \frac{\tan(\alpha) + \tan(\beta)}{1 - \tan(\alpha) \cdot  \tan(\beta)}
\end{equation}
In dieser Formel setzen wir $\alpha = \arctan(x)$ und $\beta = \arctan(y)$ ein und
erhalten
\\[0.3cm]
\hspace*{1.3cm}
$\tan(\arctan(x) + \arctan(y)) = \bruch{\tan(\arctan(x)) + \tan(\arctan(y))}{1 - \tan(\arctan(x)) \cdot  \tan(\arctan(y))}$
\\[0.3cm]
Nehmen wir nun von beiden Seiten dieser Gleichung den Arkustangens und ber\"ucksichtigen,
dass $\tan(\arctan(x)) = x$ 
und $\tan(\arctan(y)) = y$  gilt, so erhalten wir das Additions-Theorem f\"ur den
Arkustangens:
\begin{equation}
  \label{eq:arcTangensAddition}
  \arctan(x) + \arctan(y) = \arctan\left(\bruch{x + y}{1 - x \cdot  y}\right)
\end{equation}
Hier setzen wir nun $y := x$.  Das liefert
\\[0.3cm]
\hspace*{1.3cm}
$2\cdot \arctan(x) = \arctan\left(\bruch{2\cdot x}{1 - x^2}\right)$
\\[0.3cm]
Wir wollen $\arctan(1)$ berechnen.  Daher w\"ahlen wir $x$ so, dass Folgendes gilt:
\\[0.3cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lrcl}
               & \bruch{2\cdot x}{1 - x^2} &=& 1 \\[0.3cm]
\Leftrightarrow& 2\cdot x  &=& 1 - x^2 \\[0.2cm]
\Leftrightarrow& x^2 + 2\cdot x +1 &=& 2  \\[0.2cm]
\Leftarrow&x   &=& \sqrt{2} - 1  \\[0.2cm]
\end{array}
$
\\[0.3cm]
Wir k\"onnen also $x = \sqrt{2} - 1$ w\"ahlen und dann $\pi$ nach der Formel 
\begin{equation}
  \label{eq:Pi}
  \pi = 4 \cdot  \arctan(1) = 8 \cdot  \arctan\bigl(\sqrt{2} - 1\bigr) = 8 \cdot  \sum\limits_{n=0}^\infty (-1)^n \cdot  \bruch{(\sqrt{2}-1)^{2\cdot n+1}}{2\cdot n+1}
\end{equation}
berechnen.  Wegen $\sqrt{2} - 1 \approx 0.4142$ konvergiert diese Reihe recht gut.
Um auszurechnen, wieviele Glieder ben\"otigt werden um $\pi$ auf eine
Genauigkeit von $10^{-9}$ zu berechnen, sch\"atzen wir den Abbruch-Fehler mit dem
Leibniz-Kriterium ab, wobei wir zur Vereinfachung  den Nenner $2\cdot n+1$ durch 1 absch\"atzen:
\\[0.3cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lrcl}
                & 8 \cdot  \bigl(\sqrt{2}-1\bigr)^{2\cdot (n+1)+1} & \leq & 10^{-9} \\[0.2cm]
\Leftrightarrow & \ln(8) + (2\cdot n+3)\cdot \ln\bigl(\sqrt{2}-1\bigr) & \leq & -9 \cdot \ln(10) \\[0.2cm]
\Leftrightarrow &  (2\cdot n+3)\cdot \ln\bigl(\sqrt{2}-1\bigr) & \leq & -9 \cdot \ln(10) - \ln(8)\\[0.3cm]
\Leftrightarrow &  2\cdot n+3 & \geq & - \bruch{9 \cdot \ln(10) + \ln(8)}{\ln\bigl(\sqrt{2}-1\bigr)}\\[0.5cm]
\Leftrightarrow &  n & \geq &  -0.5\cdot \left(\bruch{9 \cdot \ln(10) + \ln(8)}{\ln\bigl(\sqrt{2}-1\bigr)} + 3\right) \approx 11.4
\end{array}
$
\\[0.3cm]
Also reicht es sicher aus, die ersten 12 Glieder der Summe zu ber\"ucksichtigen um $\pi$ auf eine Genauigkeit von $10^{-9}$
zu berechnen.  F\"uhren wir die Rechnung durch, so finden wir 
\\[0.2cm]
\hspace*{1.3cm}
$\ds\pi \approx \sum\limits_{k=0}^{12} (-1)^k \cdot  \bruch{(\sqrt{2}-1)^{2\cdot k+1}}{2\cdot k+1} \approx 3.141592653601609$.
\\[0.2cm]
Der tats\"achliche Fehler ist hier kleiner als $2\cdot 10^{-11}$.
\vspace*{0.3cm}

\subsubsection{Die Machin'sche Formel$^*$}
Es gibt noch eine elegantere M\"oglichkeit, die Kreiszahl $\pi$ mit Hilfe des Arkustangens zu
berechnen.  Es gilt n\"amlich die Machin'sche Formel (John Machin, 1686 -- 1751):
\\[0.2cm]
\hspace*{1.3cm}
\colorbox{red}{\colorbox{orange}{\framebox{$\ds\frac{\pi}{4} = 4 \cdot \arctan\Bigl(\frac{1}{5}\Bigr) - \arctan\Bigl(\frac{1}{239}\Bigr)$}}}

\proof
Wegen $\bruch{\pi}{4}= \arctan(1)$ ist die Machin'sche Formel \"aquivalent zu
\\[0.2cm]
\hspace*{1.3cm}
$\arctan(1) = 4 \cdot \arctan\Bigl(\bruch{1}{5}\Bigr) - \arctan\Bigl(\bruch{1}{239}\Bigr)$.
\\[0.2cm]
Wir addieren auf beiden Seiten dieser Gleichung den Wert $\arctan\bigl(\frac{1}{239}\bigr)$ und
sehen dann, dass die Machin'sche Gleichung zu der Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$\arctan(1)+ \arctan\Bigl(\bruch{1}{239}\Bigr) = 4 \cdot \arctan\Bigl(\bruch{1}{5}\Bigr)$.
\\[0.2cm]
\"aquivalent ist.  Wir wenden nun auf beiden Seiten dieser Gleichung das Additions-Theorem des
Arkustangens an und finden
\\[0.2cm]
\hspace*{1.3cm}
$\arctan\left(\cfrac{1 + \frac{1}{239}}{1 - \frac{1}{239}}\right) = 
 2 \cdot \arctan\left(\cfrac{\frac{2}{5}}{1 - \frac{1}{25}}\right)
$
\\[0.2cm]
Dies vereinfachen wir zu
\\[0.2cm]
\hspace*{1.3cm}
$\ds\arctan\Bigl(\frac{240}{238}\Bigr) =  2 \cdot \arctan\Bigl(\frac{10}{24}\Bigr)$
\\[0.2cm]
K\"urzen liefert
\\[0.2cm]
\hspace*{1.3cm}
$\ds\arctan\Bigl(\frac{120}{119}\Bigr) =  2 \cdot \arctan\Bigl(\frac{5}{12}\Bigr)$
\\[0.2cm]
Hier k\"onnen wir auf der rechten Seite das Additions-Theorem des Arkustangens ein zweites Mal
anwenden und finden
\\[0.2cm]
\hspace*{1.3cm}
$\ds\arctan\Bigl(\frac{120}{119}\Bigr) =  
 \arctan\left(\cfrac{\frac{10}{12}}{1 - \frac{25}{12}\cdot\frac{25}{12}}\right)$
\\[0.2cm]
Elementare Bruchrechnung zeigt die G\"ultigkeit der Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$\dfrac{\frac{10}{12}}{1 - \frac{25}{12}\cdot\frac{25}{12}} = \cfrac{120}{119}$.
\\[0.2cm]
Damit haben wir die Machin'sche Formel bewiesen.  \qed

\exercises
Leiten Sie die folgende Formel aus dem Additions-Theorem des
Arcus-Tangens her und berechnen Sie damit $\pi$ auf eine Genauigkeit von $10^{-9}$:
\\[0.3cm]
\hspace*{1.3cm}
$\bruch{\pi}{4} =  2 \cdot  \arctan\Bigl(\frac{1}{2}\Bigr) -\arctan\Bigl(\frac{1}{7}\Bigr) $.

\section{Polynom-Interpolation}
Nach dem wir uns im letzten Abschnitt damit besch\"aftigt haben f\"ur eine gegebene Funktion
$f$ eine Folge von Polynomen zu konstruieren, deren Ableitungen im Punkt $0$ mit der
Funktion $f$ \"ubereinstimmen, zeigen wir jetzt, wie sich Polynome konstruieren lassen, die
mit eine Funktion $f$ an vorgegebenen Punkten \"ubereinstimmen.  Sind $n+1$ Paare der Form
\\[0.2cm]
\hspace*{1.3cm}
$\pair(x_0,y_0)$,
$\pair(x_1,y_1)$,
$\cdots$,
$\pair(x_n,y_n)$
\\[0.2cm]
gegeben, so besteht die Aufgabe der
\href{https://de.wikipedia.org/wiki/Interpolation_(Mathematik)}{\emph{Polynom-Interpolation}} darin,
ein Polynom $p(x)$ vom  $n$ zu finden, so dass
\\[0.2cm]
\hspace*{1.3cm}
$\forall i \in \{0,1,\cdots,n\}: p(x_i) = y_i$
\\[0.2cm]
gilt.  Wir zeigen nun, dass diese Aufgabe l\"osbar ist:  Zu einer gegebenen Liste von 
$n+1$ verschiedenen \emph{St\"utzstellen} 
\\[0.2cm]
\hspace*{1.3cm}
$\bigl[x_0, x_1, \cdots, x_n]$
\\[0.2cm]
definieren wir das $k$-te \emph{Lagrange'sche Polynom} 
(\href{https://de.wikipedia.org/wiki/Joseph-Louis_Lagrange}{Joseph-Louis Lagrange}, 1736 -- 1813)
vom Grad $n$ f\"ur alle $k\in\{0,1,\cdots,n\}$ wie folgt: 
\begin{equation}
  \label{eq:lagrangePolynom}
\colorbox{red}{\colorbox{orange}{\framebox{$\ds L_k\bigl([x_0,\cdots,x_n];x\bigr) := \prod\limits_{i=0 \atop i\not=k}^n \bruch{x-x_i}{x_k-x_i}$}}}
\end{equation}
F\"ur die Folge der St\"utzstellen $[-1,0,1]$ lauten die Lagrange'schen Polynome beispielsweise
\begin{enumerate}
\item $L_0\bigl([-1,0,1];x\bigr) := \bruch{(x-0)\cdot (x-1)}{(-1-0)\cdot (-1-1)} = 
       \frac{1}{2}\cdot x^2 - \frac{1}{2}\cdot x$ 
\item $L_1\bigl([-1,0,1];x\bigr) := \bruch{\bigl(x-(-1)\bigr)\cdot (x-1)}{(0-(-1))\cdot (0-1)} = -x^2 + 1$
\item $L_2\bigl([-1,0,1];x\bigr) := \bruch{\bigl(x-(-1)\bigr)\cdot (x-0)}{(1-(-1))\cdot (1-0)} = \frac{1}{2}\cdot x^2 + \frac{1}{2}\cdot x$
\end{enumerate}
Ist eine Liste $[x_0,x_1,\cdots,x_n]$ von $n+1$ verschiedenene St\"utzstellen gegeben, so
haben die Lagrange'schen Polynome eine sehr n\"utzliche Eigenschaft.  Um diese
Eigenschaft einfacher schreiben zu k\"onnen, definieren wir f\"ur nat\"urliche Zahlen $j$ und
$k$ das \href{https://de.wikipedia.org/wiki/Kronecker-Delta}{\emph{Kronecker-Delta}}
(\href{https://de.wikipedia.org/wiki/Leopold_Kronecker}{Leopold Kronecker}, 1823 -- 1891) wie folgt:
\begin{equation}
  \label{eq:kroneckerDelta}
  \delta_{k,j} = \left\{ \begin{array}[c]{ll}
                          1 & \mbox{falls} \quad j = k;      \\
                          0 & \mbox{falls} \quad j \not= k.
                         \end{array}
                 \right.  
\end{equation}
Damit gilt nun
\begin{equation}
  \label{eq:lagrangeProperty}
L_k\bigl([x_0,x_1,\cdots,x_n];x_j\bigr) = \delta_{k,j}. 
\end{equation}
Diese Eigenschaft werden wir durch einfaches Nachrechnen best\"atigen.  Wir betrachten die F\"alle
$j=k$ und $j\not=k$ getrennt:
\begin{enumerate}
\item Fall: $j=k$.  Dann haben wir 
      \\[0.2cm]
      \hspace*{0.0cm}
      $\ds L_k\bigl([x_0,\cdots,x_n];x_k\bigr) = \prod\limits_{i=0 \atop i\not=k}^n
      \bruch{x_k-x_i}{x_k-x_i} = 1 = \delta_{k,k}$
\item Fall: $j\not=k$.  Dann haben wir 
      \\[0.2cm]
      \hspace*{-0.0cm}
      $
      \begin{array}[c]{lcl}
      L_k\bigl([x_0,\cdots,x_n];x_j\bigr) 
      & = & \ds\prod\limits_{i=0 \atop i\not=k}^n \bruch{x_j-x_i}{x_k-x_i} \\[0.6cm]
      & = & \ds\frac{(x_j - x_0) \cdot \mbox{$\dots$} \cdot  (x_j - x_j) \cdot \mbox{$\dots$} \cdot (x_j - x_n)}{
                     (x_k - x_0) \cdot \mbox{$\dots$} \cdot (x_k - x_j) \cdot \mbox{$\dots$} \cdot (x_k - x_n)} 
        \;=\; 0 \;=\; \delta_{j,k},
      \end{array}
      $
\end{enumerate}
denn f\"ur $j \not= k$ enth\"alt das Produkt 
\\[0.2cm]
\hspace*{1.3cm}
$\ds\prod\limits_{i=0 \atop i\not=k}^n x_j - x_i$
\\[0.2cm]
den Faktor $x_j - x_j$ und ist damit gleich $0$.
Die Eigenschaft (\ref{eq:lagrangeProperty}) macht es jetzt einfach, Polynome zu
konstruieren, die an den  St\"utzstellen $x_0,x_1,\cdots,x_n$ die vorgegebenen Werte 
$y_0$, $y_1$, $\cdots$, $y_n$ annehmen. Wir definieren 
\\[0.3cm]
\hspace*{1.3cm}
$\displaystyle p(x) := \sum\limits_{k=0}^n y_k \cdot  L_k(x)$.
\\[0.3cm]
Dann gilt $p(x_j) = y_j$ f\"ur alle $j=0,1,\cdots,n$, denn wir haben
\\[0.2cm]
\hspace*{1.3cm}
$\displaystyle p(x_j) = \sum\limits_{k=0}^n y_k \cdot  L_k(x_j) = \sum\limits_{k=0}^n y_j \cdot  \delta_{j,k} = y_j \cdot \delta_{j,j} = y_j$. 
\\[0.2cm] 
Also l\"ost das oben definierte Polynom $p(x)$ das Interpolations-Problem 
\\[0.2cm]
\hspace*{1.3cm}
$\pair(x_0, y_0)$, $\pair(x_1, y_1)$, $\cdots$, $\pair(x_n, y_n)$.


\example
Wollen wir ein Polynom $p(x)$ konstruieren, welches das Interpolations-Problem
\\[0.2cm]
\hspace*{1.3cm}
$\pair(-1,1)$, $\pair(0,0)$ und $\pair(1,1)$
\\[0.2cm]
l\"ost, so k\"onnen wir mit den oben gefundenen Lagrange'schen Polynomen das Polynom $p(x)$ wie folgt
definieren: 
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
  p(x) & = & 1 \cdot  L_0\bigl([-1,0,1];x\bigr) + 0 \cdot  L_1\bigl([-1,0,1];x\bigr) + 1\cdot L_2\bigl([-1,0,1];x\bigr) \\[0.2cm]
       & = & \bigl(\frac{1}{2}\cdot x^2 - \frac{1}{2}\cdot x\bigr) + \bigl(\frac{1}{2}\cdot x^2 + 
             \frac{1}{2}\cdot x\bigr)  \\[0.2cm]
       & = & x^2
\end{array}
$
\vspace*{0.3cm}


\exercise
Bei einer Klausur k\"onnen insgesamt $n$ Punkte erreicht werden. Bestimmen
Sie ein Polynom $p(x)$ vom Grade 1, so dass
\\[0.2cm]
\hspace*{1.3cm} $p(n) = 1.0$ \quad und \quad $\ds p\Bigl(\frac{n}{2}\Bigr) = 4.0$
\\[0.2cm]
gilt.  Hat ein Teilnehmer einer Klausur $k$ von $n$ Punkten erreicht, so ist $p(k)$ die
Note, mit der die Leistung bewertet wird.  Falls $p(k) > 5,0$ ist, wird die Leistung mit der Note
$5,0$ bewertet.
\eox

\subsection{Interpolation nach Newton$^*$}
Bei der Rechnung mit den oben definierten Lagrange'schen Polynomen tritt in der Praxis ein
Problem auf.  Hat man f\"ur eine gegebene Zahl von St\"utzstellen das Interpolations-Problem
gel\"ost und erh\"alt man nun eine zus\"atzliche St\"utzstelle, so ist es erforderlich, alle
Lagrange'schen Polynome noch einmal zu berechnen, denn die Lagrange'schen Polynome vom
Grad $n+1$ haben mit den Lagrange'schen Polynomen vom Grad $n$ nur wenig zu tun.  Hier ist
der Ansatz von Newton besser geeignet.  Bei dem Newton'schen Ansatz schreibt sich ein
Interpolations-Polynom vom Grad $n$ in der Form 
\begin{equation}
  \label{eq:newtonInterpolation}  
\begin{array}[t]{lcl}
p_n(x) & = & \ds\sum\limits_{k=0}^n c_k \cdot  \prod\limits_{i=0}^{k-1}(x-x_i) \\[0.3cm]
       & = & \ds c_0 + c_1\cdot (x-x_0) + c_2 \cdot  (x-x_0)\cdot (x - x_1) + \cdots + c_n \cdot  \prod\limits_{i=0}^{n-1}(x-x_i) 
\end{array}
\end{equation}
Der Vorteil dieses Ansatzes besteht darin, dass das Newton'sche Interpolations-Polynom vom Grad $n+1$ unmittelbar aus
dem Newton'schen Interpolations-Polynom vom Grad $n$ wie folgt hervorgeht: 
\\[0.2cm]
\hspace*{1.3cm}
$\ds p_{n+1}(x) = p_n(x) + c_{n+1} \cdot  \prod\limits_{i=0}^{n}(x-x_i)$.
\\[0.2cm]
Ist eine Interpolations-Aufgabe 
\\[0.2cm]
\hspace*{1.3cm}
$\pair(x_0,y_0)$, 
$\pair(x_1,y_1)$, 
$\cdots$, 
$\pair(x_n,y_n)$, 
\\[0.2cm]
gegeben, so k\"onnen die Koeffizienten $c_k$ f\"ur $k=0,1,\cdots,n$ der Reihe nach wie folgt
berechnet werden.
\begin{enumerate}
\item Um $c_0$ zu bestimmen, setzen wir in Gleichung (\ref{eq:newtonInterpolation}) f\"ur 
      $x$ den Wert $x_0$ ein.  Dann fallen alle Terme bis auf den ersten Term weg und wir 
      erhalten
      \\[0.2cm]
      \hspace*{1.3cm}
      $y_0 = c_0$.
\item Um $c_1$ zu bestimmen, setzen wir in Gleichung (\ref{eq:newtonInterpolation}) f\"ur 
      $x$ den Wert $x_1$ ein.  Dann fallen alle Terme bis auf die ersten beiden Term weg und wir 
      erhalten
      \\[0.2cm]
      \hspace*{1.3cm}
      $y_1 = c_0 + c_1\cdot (x_1-x_0)$.
      \\[0.2cm]
      Setzen wir hier f\"ur $c_0$ den im letzten Schritt gefundenen Wert $y_0$ ein, so finden wir 
      \\[0.2cm]
      \hspace*{1.3cm}
      $c_1 = \bruch{y_1 - y_0}{x_1 - x_0}$.
\item Um $c_2$ zu bestimmen, setzen wir in Gleichung (\ref{eq:newtonInterpolation}) f\"ur 
      $x$ den Wert $x_2$ ein.  Dann fallen alle Terme bis auf die ersten drei Terme weg und wir 
      erhalten
      \\[0.2cm]
      \hspace*{1.3cm}
      $y_2 = c_0 + c_1\cdot (x_2-x_0) + c_2\cdot (x_2-x_0)\cdot (x_2-x_1)$.
      \\[0.2cm]
      Hier setzen wir f\"ur $c_0$ und $c_1$ die in den letzten Schritten gefundenen Werte
      ein und haben dann
      \\[0.2cm]
      \hspace*{1.3cm}
      $
      \begin{array}[t]{rcl}
      y_2 & = & y_0 + \bruch{y_1 - y_0}{x_2 - x_0} \cdot  (x_2-x_0) + c_2\cdot (x_2-x_0)\cdot (x_2-x_1) \\[0.3cm]
      y_2 - y_0 & = & \bruch{y_1 - y_0}{x_2 - x_0} \cdot  (x_2-x_0) + c_2\cdot (x_2-x_0)\cdot (x_2-x_1) \\[0.3cm]
      \bruch{y_2 - y_0}{x_2-x_0} & = & \bruch{y_1 - y_0}{x_2 - x_0}  + c_2\cdot (x_2-x_1) \\[0.5cm]
      \bruch{y_2 - y_0}{x_2-x_0} - \bruch{y_1 - y_0}{x_2 - x_0} & = & c_2\cdot (x_2-x_1) \\[0.6cm]
      \bruch{\bruch{y_2 - y_0}{x_2-x_0} - \bruch{y_1 - y_0}{x_2 - x_0}}{x_2-x_1} & = & c_2 
      \end{array}
      $
\end{enumerate}
Die obige Rechnung gibt Anlass zur Definition der sogenannten 
\emph{dividierten Differenzen} vom Rang $k$, die wir jetzt f\"ur alle $k= 1,\cdots,n$ durch
Induktion \"uber  $k$ definieren.  
\begin{enumerate}
\item[I.A.:] $k=1$.  F\"ur alle $i=0,\cdots,n$ setzen wir 
             \\[0.2cm]
             \hspace*{1.3cm}
             $[x_k]_{\mathrm{dd}} := y_k$.
\item[I.S.:] $k\mapsto k+1$. F\"ur $i=0,\cdots,n-k$ setzen wir 
             \\[0.2cm]
             \hspace*{1.3cm}
             $[x_i,x_{i+1},\cdots,x_{i+k}]_{\mathrm{dd}} := \bruch{[x_{i+1},\cdots,x_{i+k}]_{\mathrm{dd}} - [x_{i},\cdots,x_{i+k-1}]_{\mathrm{dd}}}{x_{i+k}-x_i}$.
\end{enumerate}
Die dividierten Differenzen der Ordnung $k+1$ berechnen sich also aus den dividierten
Differenzen der Ordnung $k$ durch Bildung einer Differenz und einer anschlie{\ss}enden
Division.  Dieser Umstand erkl\"art ihren Namen.  F\"ur die Koeffizienten $c_k$ in dem
Newton'schen Ansatz (\ref{eq:newtonInterpolation}) gilt nun 
\begin{equation}
  \label{eq:newtonInterpolationCk}
  c_k = [x_0,x_1,\cdots,x_k]_{\mathrm{dd}},
\end{equation}
das Interpolations-Problem ein Polynom $p(x)$ zu finden, f\"ur das
\\[0.2cm]
\hspace*{1.3cm}
$p(x_0) = y_0$, \quad
$p(x_1) = y_1$, \quad $\cdots$ \quad und \quad
$p(x_n) = y_n$
\\[0.2cm]
gilt, wird also durch das Polynom
\begin{equation}
  \label{eq:newtonInterpolationFinal}
\colorbox{red}{\colorbox{orange}{\framebox{$\ds p(x) \; = \; \sum\limits_{k=0}^n [x_0,x_1,\cdots,x_k]_{\mathrm{dd}} \cdot  \prod\limits_{i=0}^{k-1}(x-x_i)$}}}
\end{equation}
gel\"ost.
\subsection{Der Interpolations-Fehler}
Wir untersuchen als n\"achstes, wie gro{\ss} der Fehler bei der Polynom-Interpolation werden
kann.  Dazu beweisen wir zun\"achst den folgenden Hilfs-Satz.  Dieser Satz ist eine Verallgemeinerung
des Satzes von Rolle (Satz \ref{satz:rolle}). 

\begin{Satz}
  Ist $f:\mathbb{R} \rightarrow \mathbb{R}$ eine Funktion, die $n$-mal differenzierbar
  ist und die au{\ss}erdem $n+1$ verschiedene Null-Stellen 
  \\[0.2cm]
  \hspace*{1.3cm}  $x_0 < x_1 < \cdots < x_n$
  \\[0.2cm]
  hat, so gibt es ein $\xi\in[x_0,x_n]$ mit $f^{(n)}(\xi) = 0$.
\end{Satz}
\textbf{Beweis}:  Wir zeigen, dass f\"ur alle $k=0,1,\cdots,n$ die $k$-te Ableitung
$f^{(k)}(x)$ in dem Interval $[x_0,x_n]$ mindestens $n+1-k$ verschiedene Nullstellen hat.
Diesen Nachweis f\"uhren wir durch Induktion \"uber $k$.
\begin{enumerate}
\item[I.A.:] $k=0$. Es gilt $f^{(0)}(x) = f(x)$ und da die Funktion $f$ nach Voraussetzung $n+1$
             verschiedene Nullstellen hat, folgt die Behauptung.
\item[I.S.:] $k\mapsto k+1$.  Nach Induktions-Voraussetzung hat die Funktion
             $f^{(k)}(x)$ mindestens $n + 1 -k$ verschiedene Nullstellen.
             Nehmen wir an, diese Nullstellen seien der Gr\"o{\ss}e nach geordnet als
             \\[0.2cm]
             \hspace*{1.3cm} $y_1 < y_2 < \cdots y_{n+1-k}$. \\[0.2cm]
             Wegen $k+1\leq n$ ist die Funktion $f^{(k)}(x)$ nach Voraussetzung
             differenzierbar und nach dem Satz von Rolle hat die Ableitung dieser Funktion
             jeweils zwischen zwei Nullstellen $y_i$ und $y_{i+1}$ eine Nullstelle, 
             f\"ur $i=1,\cdots,n-k-1$ gibt es also $z_i \in (y_i,y_{i+1}) \subseteq[x_0,x_n]$ mit 
             \\[0.2cm]
             \hspace*{1.3cm} $\dfo f^{(k)}(z_i) = f^{(k+1)}(z_i) = 0$.
\end{enumerate}
Setzen wir in der gerade bewiesenen Behauptung f\"ur $k$ den Wert $n$ ein, so sehen wir, dass die Funktion $f^{(n)}(x)$ in dem Interval
$[x_0,x_n]$ mindestens $n+1-n=1$ Nullstelle hat, also gibt es das gesuchte
$\xi\in[x_0,x_n]$.
\hspace*{\fill} $\Box$

\exercise
Es sei $p(x)$  ein Polynom vom Grad $n\geq 1$.  Zeigen Sie, dass
$p(x)$ h\"ochstens $n$ verschiedene Nullstellen hat.  Folgern Sie daraus, dass es zu $n$ gegebenen Paaren 
\\[0.2cm]
\hspace*{1.3cm}
$\pair(x_0,y_0)$, $\pair(x_1,y_1)$, $\cdots$, $\pair(x_n,y_n)$,
\\[0.2cm]
genau ein Polynom $p(x)$ vom Grad $\leq n$ gibt, so dass $p(x_i) = y_i$ f\"ur alle $i=0,1,\cdots,n$ gilt. \eox


\begin{Satz} 
  \label{satz:interpolationsFehler}
  Ist die Funktion $f:\mathbb{R} \rightarrow \mathbb{R}$ mindestens $(n+1)$-mal differenzierbar, ist $p(x)$ eine Polynom vom Grad
  kleiner gleich $n$, sind $x_0 < x_1 < \cdots < x_n$ Punkte mit
  \\[0.2cm]
  \hspace*{1.3cm}
  $f(x_i) = p(x_i)$ f\"ur alle $i=0,1,\cdots,n$,
  \\[0.2cm]
  und ist $\bar{x} \in [x_0,x_n]$, so gibt es ein $\zeta\in [x_0,x_n]$, so dass f\"ur
  den Interpolations-Fehler $f(\bar{x}) - p(\bar{x})$ gilt: 
  \\[0.2cm]
  \hspace*{1.3cm}
\colorbox{red}{\colorbox{orange}{\framebox{$\ds f(\bar{x}) - p(\bar{x}) = \bruch{f^{(n+1)}(\zeta)}{(n+1)!} \cdot  \prod\limits_{i=0}^{n}(\bar{x}-x_i)$.}}}
\end{Satz}

\noindent
\textbf{Beweis}: Falls $\bar{x} \in \{x_0,x_1,\cdots,x_n\}$ ist, dann folgt sofort
$f(\bar{x}) - p(\bar{x}) = 0$ und da dann auch 
\\[0.2cm]
\hspace*{1.3cm}
 $\ds\prod\limits_{i=0}^{n}(\bar{x}-x_i) = 0$ 
\\[0.2cm]
gilt, ist die Behauptung in diesem Fall offensichtlich.  Andernfalls 
definieren wir die Funktion 
\\[0.2cm]
\hspace*{1.3cm}
$\ds g(x) := f(x) - p(x) - \left(\prod\limits_{i=0}^{n} \bruch{x-x_i}{\bar{x}-x_i}\right) \cdot  \bigl(f(\bar{x}) - p(\bar{x})\bigr)$.
\\[0.2cm]
Mit $f$ ist auch die Funktion $g$ mindestens $(n+1)$-mal differenzierbar.  Au{\ss}erdem gilt
wegen $p(x_k) = f(x_k)$ f\"ur alle $k=0,1,\cdots,n$
\\[0.2cm]
\hspace*{1.3cm}
$\ds g(x_k) = f(x_k) - p(x_k) - \left(\prod\limits_{i=0}^{n} \bruch{x_k-x_i}{\bar{x}-x_i}\right) \cdot  \bigl(f(\bar{x}) - p(\bar{x})\bigr) = 0$,
\\[0.2cm]
denn der Faktor $x_k-x_i$ verschwindet im Falle $i=k$. Au{\ss}erdem haben wir 
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
  g(\bar{x}) & = & \ds f(\bar{x}) - p(\bar{x}) - \left(\prod\limits_{i=0}^{n} \bruch{\bar{x}-x_i}{\bar{x}-x_i}\right) \cdot  \bigl(f(\bar{x}) - p(\bar{x})\bigr) \\[0.5cm]
             & = & \ds f(\bar{x}) - p(\bar{x}) - 1\cdot \bigl(f(\bar{x}) - p(\bar{x})\bigr) \\[0.2cm]
             & = & 0.
\end{array}
$
\\[0.2cm]
Damit hat die Funktion insgesamt $n+2$ verschiedene Nullstellen.  Wenden wir jetzt auf die
Funktion $g$ den eben gezeigten Hilfs-Satz an, so finden wir
ein $\zeta\in [x_0,x_n]$  mit 
\\[0.2cm]
\hspace*{1.3cm} $g^{(n+1)}(\zeta) = 0$. 
\\[0.2cm]
Wir bilden die $(n+1)$-te Ableitung von $g$ und finden 
\begin{equation}
  \label{eq:polynomAbschaetzung}
g^{(n+1)}(x) = f^{(n+1)}(x) - 0 - (n+1)!\cdot  \left(\prod\limits_{i=0}^{n} \bruch{1}{\bar{x} - x_i}\right)\cdot  \bigl(f(\bar{x}) - p(\bar{x})\bigr),
\end{equation}
denn die $(n+1)$-te Ableitung eines Polynoms vom Grad $n$ ist $0$ und die $(n+1)$-te
Ableitung des Polynoms 
\\[0.2cm]
\hspace*{1.3cm} $\displaystyle\prod\limits_{i=0}^{n} \bruch{x-x_i}{\bar{x}-x_i}$ 
\\[0.2cm]
ist $(n+1)!$ mal der Koeffizient der Potenz $x^{n+1}$.  Setzen
wir in Gleichung (\ref{eq:polynomAbschaetzung}) die Nullstelle $\zeta$ ein, so finden wir 
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[b]{ll}
                & 0 = \displaystyle f^{(n+1)}(\zeta) - (n+1)!\cdot  \left(\prod\limits_{i=0}^{n} \bruch{1}{\bar{x} - x_i}\right)\cdot  \bigl(f(\bar{x}) - p(\bar{x})\bigr) \\[0.5cm]
\Leftrightarrow & \displaystyle (n+1)!\cdot  \left(\prod\limits_{i=0}^{n} \bruch{1}{\bar{x} - x_i}\right)\cdot  \bigl(f(\bar{x}) - p(\bar{x})\bigr) =  f^{(n+1)}(\zeta) \\[0.5cm]
\Leftrightarrow & \displaystyle f(\bar{x}) - p(\bar{x}) =  \bruch{f^{(n+1)}(\zeta)}{(n+1)!} \cdot  \prod\limits_{i=0}^{n} (\bar{x} - x_i).\\[0.5cm]
\end{array}
$ \hspace*{\fill} $\Box$

\exercise
F\"ur die Funktion $x \mapsto \sin(x)$ soll im Intervall
$[0,\frac{\pi}{2}]$ ein Tabelle erstellt werden, so dass der bei linearer Interpolation entstehende
Interpolations-Fehler kleiner als $10^{-5}$ ist.  Das Intervall $[0,\frac{\pi}{2}]$ soll zu diesem
Zweck in gleich gro{\ss}e Intervalle aufgeteilt werden.  Berechnen Sie die Anzahl der
Eintr\"age, die f\"ur die Erstellung der Tabelle notwendig ist. \eox


\section{Der Banach'sche Fixpunkt-Satz}
Der \href{http://de.wikipedia.org/wiki/Banachscher_Fixpunktsatz}{Banach'sche Fixpunkt-Satz}, der 1922 von 
\href{http://en.wikipedia.org/wiki/Stefan_Banach}{Stefan Banach} (1892 -- 1945) bewiesen wurde, ist ein
wichtiges Hilfsmittel zur L\"osung von Gleichungen.  Wir werden den Banach'schen
Fixpunkt-Satz nur f\"ur den Spezialfall der reellen Zahlen formulieren und beweisen.
In der Mathematik wird dieser Satz in einem abstrakteren Rahmen verwendet, die Menge der
reellen Zahlen wird dann durch einen \emph{vollst\"andigen}
\href{http://de.wikipedia.org/wiki/Metrischer_Raum}{\emph{metrischen Raum}} ersetzt.

Bevor wir den Banach'schen Fixpunkt-Satz formulieren und beweisen, wollen wir das damit verbundene
Fixpunkt-Verfahren motivieren.  Wir haben bereits einmal ein solches Verfahren angewendet:  In dem Kapitel
\ref{chapter:folgen-und-reihen} \"uber Folgen und Reihen hatten wir die Gleichung 
\\[0.2cm]
\hspace*{1.3cm}
$x = \cos(x)$
\\[0.2cm]
mit Hilfe eines Fixpunkt-Verfahrens gel\"ost.  Wir hatten damals induktiv eine Folge $\folge{x_n}$
definiert, indem wir $x_1 := 0$ und $x_{n+1} := \cos(x_n)$ definiert hatten.  Das in Abbildung
\ref{fig:solve.stlx} auf Seite \pageref{fig:solve.stlx}
gezeigte Programm berechnet die Folge $\folge{x_n}$ und wir hatten gesehen,
dass diese Folge gegen einen Grenzwert 
\\[0.0cm]
\hspace*{1.3cm}
$\ds \bar{x} := \lim\limits_{n\rightarrow\infty} x_n$ 
\\[0.2cm]
konvergiert,  der wegen
\\[0.2cm]
\hspace*{1.3cm}
$\cos(\bar{x}) = \cos\left(\lim\limits_{n\rightarrow\infty} x_n\right) = 
\lim\limits_{n\rightarrow\infty} \cos(x_n) = \lim\limits_{n\rightarrow\infty} x_{n+1} = \bar{x}
$
\\[0.2cm]
auch eine L\"osung der Fixpunkt-Gleichung $x = \cos(x)$ ist.  Wir wollen dieses Verfahren nun
verallgemeinern.  Wir nehmen dazu an, dass eine stetige Funktion $f$ gegeben ist und wir eine L\"osung
der Fixpunkt-Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$x = f(x)$ 
\\[0.2cm]
iterativ bestimmen wollen, indem wir induktiv eine Folge $\folge{x_n}$ definieren, wobei wir
ausgehend von einem weitgehend beliebig gew\"ahlten Startwert $x_1$ die \"ubrigen Folgenglieder
$x_{n}$ induktiv durch die Festlegung 
\\[0.2cm]
\hspace*{1.3cm}
$x_{n+1} = f(x_n)$ 
\\[0.2cm]
definieren.  Sollte die Folge $\folge{x_n}$ gegen einen Grenzwert $\bar{x}$ konvergieren, so ist
dieser Grenzwert eine L\"osung der Fixpunkt-Gleichung $x = f(x)$, denn es gilt
\\[0.2cm]
\hspace*{1.3cm}
$\ds f(\bar{x}) = f\left(\lim\limits_{n\rightarrow\infty} x_n\right) = 
\lim\limits_{n\rightarrow\infty} f(x_n) = \lim\limits_{n\rightarrow\infty} x_{n+1} = \bar{x}
$.
\\[0.2cm]
Dabei haben wir bei der Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$ f\left(\lim\limits_{n\rightarrow\infty} x_n\right) = \lim\limits_{n\rightarrow\infty} f(x_n)$
\\[0.2cm]
die Stetigkeit von $f$ ausgenutzt.  Weniger klar ist die Antwort auf die Frage, wann diese
 Folge $\folge{x_n}$ konvergiert.  Die entscheidende Antwort auf diese Frage hat
\href{http://de.wikipedia.org/wiki/Stefan_Banach}{Stefan Banach} gegeben:  Wenn die Funktionswerte
von $f$ n\"aher beieinander liegen als die Argumente, 
dann konvergiert die Folge.  Bevor wir das im Detail formal untersuchen, wollen wir die dahinter
liegende Anschauung verstehen.  Nehmen wir an, dass die Fixpunkt-Gleichung $f(x) = x$ eine L\"osung
$\bar{x}$ hat und nehmen wir weiter an, dass eine N\"aherung $x_1$ f\"ur 
$\bar{x}$ gegeben ist.  Wenn nun die Funktionswerte von $f$ n\"aher beieinander liegen als die
Argumente, dann k\"onnen wir die N\"aherung $x_1$ zu einer N\"aherung $x_2$ verbessern, indem wir 
\\[0.2cm]
\hspace*{1.3cm}
$x_2 := f(x)$
\\[0.2cm]
definieren.  Warum ist $x_2$ besser als $x_1$?  Um diese Frage zu beantworten, betrachten wir den
Abstand von  $x_2$ und $\bar{x}$.  Es gilt
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}{cll}
   & |x_2 - \bar{x}|  \\[0.2cm]
 = & |f(x_1) - f(\bar{x})| & \mbox{denn $x_2 = f(x_1)$ und $\bar{x} = f(\bar{x})$}  \\[0.2cm]
 < & |x_1 - \bar{x}|,
\end{array}
$
\\[0.2cm]
wobei wir bei der letzten Ungleichung benutzt haben, dass die Funktionswerte von $f$ n\"aher
beieinander liegen als die Argumente.  Um diese Argumentation wasserdicht zu machen, definieren wir
zun\"achst formal unter welchen Umst\"anden die Funktionswerte einer Funktion n\"aher beieinander liegen
als die Argumente.  Die nun folgende Definition ist etwas sch\"arfer, als Sie es vielleicht im ersten
Moment vermuten w\"urden.  Das werden wir sp\"ater noch diskutieren.
\pagebreak

\begin{Definition}[kontrahierend] \lb
Eine Funktion $f:[a,b] \rightarrow [a,b]$ ist eine \colorbox{orange}{\emph{kontrahierende Abbildung}} wenn es eine
reelle Zahl $q$ mit $q < 1$ gibt, so dass 
\\[0.2cm]
\hspace*{1.3cm}
\colorbox{red}{\colorbox{orange}{\framebox{$\forall x,y \in [a,b]: |f(x) - f(y)| \leq q \cdot  |x - y|$}}}  
\\[0.2cm]
gilt.  Wir bezeichnen die Zahl $q$ als den \colorbox{orange}{\emph{Kontraktions-Koeffizienten}}.
\eod
\end{Definition}

\noindent
\textbf{Beispiel}: Die Funktion
\\[0.2cm]
\hspace*{1.3cm}
$\cos:[0,1] \rightarrow [0,1]$
\\[0.2cm]
ist eine kontrahierende Abbildung.  
Zun\"achst m\"ussen wir uns davon \"uberzeugen, dass diese Funktion wohldefiniert ist.
Dazu muss aus $x\in[0,1]$ folgen, dass auch $\cos(x) \in [0,1]$ gilt.  Dies folgt aus den
Gleichungen
\\[0.2cm]
\hspace*{1.3cm}
$\cos(0) = 1$ \quad und \quad $\cos(1) \approx 0.54$ 
\\[0.2cm]
zusammen mit der Tatsache, dass die Kosinus-Funktion in dem Intervall
$[0,1]$ monoton fallend ist, denn es gilt
\\[0.2cm]
\hspace*{1.3cm}
 $\dfo \cos(x) = -\sin(x)$  \quad und f\"ur alle $x \in \bigl[0,\pi\bigr]$ gilt $\sin(x) \geq 0$.
\\[0.2cm]
 Seien nun Zahlen $x,y\in[0,1]$ gegeben. 
Nach dem Mittelwert-Satz der Differenzial-Rechnung gibt es dann ein $\zeta\in[x,y]$ mit 
\\[0.2cm]
\hspace*{1.3cm} $\bruch{\cos(x) - \cos(y)}{x - y} = \cos'(\zeta) = - \sin(\zeta)$.
\\[0.2cm]
Die Sinus-Funktion nimmt in dem Intervall $[0,1]$ ihr Maximum in dem Punkt $1$ an, es gilt
\\[0.2cm]
\hspace*{1.3cm} $\forall t \in [0,1]: \sin(t) \leq \sin(1) \approx 0.8414709848\cdots \leq 0.85$.
\\[0.2cm]
Also haben wir folgende Absch\"atzung
\\[0.2cm]
\hspace*{1.3cm}
$|\cos(x) - \cos(y)| = \sin(\zeta) \cdot  |x - y| \leq 0.85 \cdot  |x - y|$ \quad f\"ur alle $x,y\in[0,1]$.
\eox
\vspace*{0.2cm}

\noindent
Das letzte Beispiel verallgemeinern wir zu einem Satz.

\begin{Satz}
Ist die Funktion $f:[a,b] \rightarrow [a,b]$ in dem Intervall $[a,b]$ differenzierbar und gibt es eine Zahl
$q < 1$, so dass
\\[0.2cm]
\hspace*{1.3cm}
$\forall t \in [a,b]: |f'(t)| \leq q$
\\[0.2cm]
gilt, dann ist die Abbildung $f$ kontrahierend mit dem Kontraktions-Koeffizienten $q$.
\end{Satz}

\proof
Es seien $x, y \in [a,b]$.
Nach dem Mittelwert-Satz der Differenzial-Rechnung gibt es dann ein $\zeta\in[x,y]$ mit 
\\[0.2cm]
\hspace*{1.3cm}
$\ds\frac{f(x) - f(y)}{x - y} = f'(\zeta)$.
\\[0.2cm]
Nehmen wir auf beiden Seiten dieser Ungleichung den Betrag, so erhalten wir
\\[0.2cm]
\hspace*{1.3cm}
$\ds\frac{|f(x) - f(y)|}{|x - y|} = |f'(\zeta)| \leq q$,
\\[0.2cm]
denn wir hatten ja vorausgesetzt, dass die Ungleichung $|f'(t)| < q$ f\"ur alle
$t \in [a,b]$ gilt.  Multiplizieren wir diese Ungleichung mit $|x - y|$, so erhalten wir
\\[0.2cm]
\hspace*{1.3cm}
$\ds |f(x) - f(y)| \leq q \cdot |x  - y|$
\\[0.2cm]
und nach Definition einer kontrahierenden Abbildung ist das die Behauptung.
\qed
\pagebreak

\begin{Satz}
  Ist $f:[a,b] \rightarrow [a,b]$ eine kontrahierende Abbildung, so ist $f$ auch stetig.
\end{Satz}

\exercise
Beweisen Sie den letzten Satz.

\hint
Verwenden Sie die $\varepsilon$-$\delta$-Definition der Stetigkeit.
\eox

\begin{Satz}[Banach'scher Fixpunkt-Satz] \lb
Es sei $f:[a,b] \rightarrow [a,b]$ eine kontrahierende Abbildung mit dem
Kontraktions-Koeffizienten $q$ und $x_0$ sei eine Zahl aus dem Intervall $[a,b]$.  
Definieren wir die Folge $\folge{x_n}$ induktiv durch
\\[0.2cm]
\hspace*{1.3cm} $x_{n+1} := f(x_n)$,
\\[0.2cm]
so konvergiert diese Folge.  Setzen wir 
\\[0.2cm]
\hspace*{1.3cm}
$\ds\bar{x} := \lim\limits_{n\rightarrow\infty} x_n$,
\\[0.2cm]
so gilt $f(\bar{x}) = \bar{x}$ und dar\"uber hinaus gilt die Absch\"atzung 
\\[0.2cm]
\hspace*{1.3cm}
\colorbox{red}{\colorbox{orange}{\framebox{$\ds |x_n - \bar{x}| \leq \frac{q^n}{1- q} \cdot  |x_1 - x_0|$.}}}
\end{Satz}

\proof
Wir starten den Beweis mit einer im
ersten Moment skurril anmutenden Formel: 
\\[0.2cm]
\hspace*{0.8cm}
$\ds x_n - x_0 = (x_n - x_{n-1}) + (x_{n-1} - x_{n-2}) + \cdots + (x_2 - x_1) + (x_1 - x_0) = \sum\limits_{i=1}^n (x_i - x_{i-1})$.
\\[0.2cm]
Diese Summe wird als \colorbox{orange}{\emph{Teleskop-Summe}} bezeichnet.
Daraus folgt sofort 
\\[0.2cm]
\hspace*{1.3cm}
$\ds x_n = x_0 + \sum\limits_{i=1}^n (x_i - x_{i-1})$.
\\[0.2cm]
Damit gilt dann aber
\\[0.2cm]
\hspace*{1.3cm}
$\ds\lim\limits_{n\rightarrow\infty} x_n = x_0 + \sum\limits_{i=1}^\infty (x_i - x_{i-1})$,
\\[0.2cm]
wenn wir noch zeigen k\"onnen, dass
die auf der rechten Seite dieser Formel auftretende Reihe konvergiert.  Dazu zeigen wir, dass die geometrische Reihe eine Majorante dieser Reihe ist.
Konkret zeigen wir durch vollst\"andige Induktion, dass f\"ur alle $i\in\mathbb{N}_0$ die Ungleichung
\\[0.2cm]
\hspace*{1.3cm}
$\ds|x_{i+1} - x_i| \leq q^i \cdot |x_1 - x_0|$
\\[0.2cm]
gilt.  Der Induktions-Anfang $i = 0$ ist trivial.  Im Induktions-Schritt haben wir
\\[0.2cm]
\hspace*{0.8cm}
$\ds|x_{i+2} - x_{i+1}| = |f(x_{i+1}) - f(x_i)| \leq q \cdot  |x_{i+1} - x_i| \stackrel{\mbox{\scriptsize IV}}{\leq}
q \cdot  q^i \cdot  |x_1 - x_0| = q^{i+1} \cdot  |x_1 - x_0|$.
\\[0.3cm]
Nachdem wir jetzt wissen, dass die Folge $\folge{x_n}$ konvergiert, zeigen wir, dass der
Grenzwert $\bar{x}$ dieser Folge ein Fixpunkt der Funktion $f$ ist.  Da $f$ als kontrahierende
Abbildung auch stetig ist, k\"onnen wir die Anwendung der Funktion $f$ mit der Grenzwert-Bildung
vertauschen.  Also haben wir 
\\[0.2cm]
\hspace*{1.3cm}
$\ds f(\bar{x}) = f\Bigl(\lim\limits_{n\rightarrow\infty} x_n\Bigr) =
\lim\limits_{n\rightarrow\infty} f(x_n) = \lim\limits_{n\rightarrow\infty} x_{n+1} =
\lim\limits_{n\rightarrow\infty} x_{n} = \bar{x}$
\\[0.2cm]
und dies zeigt, dass $\bar{x}$ ein Fixpunkt der Funktion $f$ ist.  Den Abstand zwischen $\bar{x}$
und $x_n$ k\"onnen wir absch\"atzen, wenn wir $\bar{x}$ als unendliche Reihe schreiben und gleichzeitig
$x_n$ als Teleskop-Summe darstellen: 
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lclcl}
       |\bar{x} - x_n| 
& = & \multicolumn{3}{l}{\ds\left|\, x_0 + \sum\limits_{i=1}^\infty (x_i - x_{i-1}) - \Bigl(x_0 + \sum\limits_{i=1}^n (x_i - x_{i-1}) \Bigr) \,\right|} \\[0.6cm]
& = & \ds \left|\, \sum\limits_{i=n+1}^\infty (x_i - x_{i-1}) \,\right| & \leq & \ds\sum\limits_{i=n+1}^\infty \bigl|x_i - x_{i-1}\bigr| \\[0.6cm]
& = & \ds \sum\limits_{i=n}^\infty \bigl|x_{i+1} - x_{i}\bigr| & \leq & \ds\sum\limits_{i=n}^\infty q^i \cdot  \bigl|x_1 - x_0\bigr| \\[0.5cm]
& = & \ds \bigl|x_1 - x_0\bigr| \cdot  \sum\limits_{i=n}^\infty q^i & = & \ds\bigl|x_1 - x_0\bigr| \cdot  \sum\limits_{i=0}^\infty q^{n+i} \\[0.5cm]
& = & \ds \bigl|x_1 - x_0\bigr| \cdot  q^{n} \cdot  \sum\limits_{i=0}^\infty q^i & = & \ds\bigl|x_1 - x_0\bigr| \cdot  q^{n} \cdot  \bruch{1}{1 - q} \\[0.5cm]
\end{array}
$
\\[0.2cm]
Die obige Ungleichungskette zeigt insgesamt die G\"ultigkeit der Absch\"atzung 
\\[0.2cm]
\hspace*{1.3cm}
$\ds|\bar{x} - x_n| \leq \frac{q^n}{1 - q} \cdot  |x_1 - x_0|$
\\[0.2cm]
und damit ist der Beweis ist abgeschlossen. \hspace*{\fill} $\Box$
\vspace*{0.3cm}

Setzen wir in der eben gezeigten Absch\"atzung f\"ur  $n$ den Wert $1$ ein, so erhalten wir 
\\[0.2cm]
\hspace*{1.3cm}
$|\bar{x} - x_1| \leq \bruch{q}{1 - q} \cdot |x_1 - x_0|$. 
\\[0.2cm]
Wenn wir in dieser Ungleichung $x_1$ durch $x_{m}$ und $x_0$ durch $x_{m-1}$ ersetzen,
beh\"alt die Ungleichung ihre G\"ultigkeit, denn wir k\"onnen $x_{m-1}$  ja als den
Startwert einer neuen Folge $\folge{y_n}$ ansehen, f\"ur die wir $y_0 = x_{m-1}$ und 
$y_1 = f(y_0) = f(x_{m-1}) = x_{m}$ setzen.  Wir haben dann also 
\\[0.2cm]
\hspace*{1.3cm}
\colorbox{red}{\colorbox{orange}{\framebox{$|\bar{x} - x_{m}| \leq \bruch{q}{1 - q} \cdot  |x_m - x_{m-1}|$.}}}
\\[0.2cm]
Wenn wir $q$ kennen, k\"onnen wir damit die G\"ute der bisher erreichten Approximation
absch\"atzen.

\example
 Wir haben oben gesehen, dass die Abbildung 
$\cos: [0,1] \rightarrow [0,1]$ kontrahierend ist mit einem Kontraktions-Koeffizienten 
$q\leq 0.85$.  Wollen wir den Fixpunkt dieser Abbildung auf eine Genauigkeit von $\varepsilon$
berechnen, so m\"ussen wir folglich solange iterieren, bis 
\\[0.2cm]
\hspace*{1.3cm} $\bruch{q}{1 - q} \cdot  |x_m - x_{m-1}| \leq \varepsilon$,
\\[0.2cm]
und das ist \"aquivalent zu
\\[0.2cm]
\hspace*{1.3cm} $|x_m - x_{m-1}| \leq \bruch{1 - q}{q} \cdot  \varepsilon$,
\\[0.2cm]
Setzen wir hier $q= 0.85$ und $\varepsilon = 10^{-6}$, so ist die Abbruchbedingung also 
\\[0.2cm]
\hspace*{1.3cm} $|x_m - x_{m-1}| \leq \bruch{1- 0.85}{0.85} \cdot  10^{-6} \approx 1.77\cdot  10^{-7}$.
\\[0.2cm]
Wir k\"onnen auch die Zahl der Iterationen absch\"atzen, die notwendig sind um den Fixpunkt
mit einer Genauigkeit von $\varepsilon$ zu berechnen.  Wir gehen dazu von der Ungleichung 
\\[0.2cm]
\hspace*{1.3cm}
$\ds |x_n - \bar{x}| \leq \frac{q^n}{1- q} \cdot  |x_1 - x_0|$.
\\[0.2cm]
aus.  Starten wir die Fixpunkt-Iteration mit $x_0 = 0$, so erhalten wir $x_1 = \cos(0) =
1$ und damit k\"onnen wir die Anzahl der Iterationen $n$ absch\"atzen: 
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
                & \ds|x_n - \bar{x}| \leq \varepsilon \\[0.2cm]
\Leftarrow      & \ds \frac{q^n}{1- q} \cdot  |x_1 - x_0| \leq \varepsilon \\[0.3cm]
\Leftrightarrow & \ds \frac{q^n}{1- q} \cdot  |1 - 0| \leq \varepsilon \\[0.3cm]
\Leftrightarrow & \ds \frac{q^n}{1- q} \leq \varepsilon \\[0.3cm]
\Leftrightarrow & \ds n \cdot  \ln(q) - \ln(1 - q) \leq \ln(\varepsilon) \\[0.3cm]
\Leftrightarrow & \ds n \cdot  \ln(q) \leq \ln(\varepsilon) + \ln(1 - q) \\[0.3cm]
\Leftrightarrow & \ds n \geq \frac{\ln(\varepsilon) + \ln(1 - q)}{\ln(q)} \\[0.5cm]
\Leftrightarrow & \ds n \geq \frac{\ln(10^{-6}) + \ln(1 - 0.85)}{\ln(0.85)} \\[0.5cm]
\Leftrightarrow & \ds n \geq \frac{-6 \cdot  \ln(10) + \ln(1 - 0.85)}{\ln(0.85)} \\[0.5cm]
\Leftrightarrow & \ds n \geq 96.7
\end{array} 
$
\\[0.2cm]
Damit ben\"otigen wir also h\"ochstens $97$ Iterationen um die gew\"unschte Genauigkeit zu erzielen.
Wir haben die einzelnen Werte der Folge, die sich bei der iterativen L\"osung der Gleichung
$x = \cos(x)$ ergibt, in Tabelle \ref{tab:x-cos-x} auf Seite \pageref{tab:x-cos-x}
angegeben.  Diese Tabelle zeigt, dass bereits nach 36 Iterationen eine Genauigkeit von
$10^{-6}$ erreicht ist.  Der Grund daf\"ur, dass es deutlich schneller ging, als wir mit der
obigen Absch\"atzung berechnet haben, liegt darin, dass der Kontraktions-Koeffizient $q$ den
Wert von $\sin(x)$ in dem Intervall $[0,1]$ absch\"atzen muss.  Da die Sinus-Funktion in
diesem Intervall monoton w\"achst, haben wir den Kontraktions-Koeffizient als 
$\sin(1) \approx 0.85$ berechnet.  F\"ur die  L\"osung $\bar{x}$ der Fixpunkt-Gleichung gilt
aber $\sin(\bar{x}) \approx 0.67$, so dass der Kontraktions-Koeffizient kleiner wird, je
mehr wir uns der L\"osung ann\"ahern.


\exercise
L\"osen Sie f\"ur $y=10^6$ und $y=10^{-6}$ die Gleichung $x\cdot \exp(x) = y$ 
durch eine einfache Fixpunkt-Iteration.  

\hint
Diese Aufgabe ist in erster Linie als Programmier-Aufgabe gedacht.  Es ist nicht gefordert, dass Sie
den Abbruchfehler untersuchen.
\eox

\remark
Bei der Definition einer kontrahierenden Abbildung haben Sie sich vielleicht gewundert, warum es
nicht reicht zu fordern, dass
\\[0.2cm]
\hspace*{1.3cm}
$|f(x) - f(y)| < |x - y|$
\\[0.2cm]
gilt.  Eine Funktion, die nur diese schw\"achere Bedingung erf\"ullt, wollen wir als eine 
\\[0.2cm]
\hspace*{1.3cm}
\colorbox{orange}{\emph{schwach kontrahierende Abbildung}} 
\\[0.2cm]
bezeichnen.  In der Tat kann gezeigt werden, dass auch f\"ur
eine schwach kontrahierende Abbildung $f$ die Folge $x_{n+1} := f(x_n)$ f\"ur beliebige Startwerte
gegen eine L\"osung der Fixpunkt-Gleichung $x = f(x)$ konvergiert.  Allerdings ist die Konvergenz unter Umst\"anden
nur sehr langsam.  Als Beispiel betrachten wir die Funktion $x \mapsto \sin(x)$.  Offenbar hat die Gleichung
$x = \sin(x)$ bei $\bar{x}=0$ einen Fixpunkt.  Setzen wir $x_1 := 1$ und berechnen wir die Folge
$x_n$ numerisch, so erhalten wir die in Tabelle \ref{tab:sinXisX} gezeigten Ergebnisse.  Selbst nach
$10^8$ Iterationen haben wir die L\"osung der Gleichung $x = \sin(x)$ erst auf drei Stellen genau
berechnet.  Der Grund daf\"ur ist, dass die Ableitung der Sinus-Funktion der Cosinus ist, und dieser hat
an der Stelle $x = 0$ den Betrag $1$.  Es ist zwar so, dass die Funktion $x \mapsto \sin(x)$
schwach kontrahierend ist, aber es gibt kein $q < 1$, so dass f\"ur alle $x,y \in \mathbb{R}$ die Ungleichung
\\[0.2cm]
\hspace*{1.3cm}
$| \sin(x) - \sin(y) | \leq q \cdot |x - y|$
\\[0.2cm]
erf\"ullt ist.  Das ist der Grund f\"ur die \"au{\ss}erst langsame Konvergenz der Folge $\folge{x_n}$.
\eox 

\begin{table}
  \centering
  \begin{tabular}{|l|l|}
    \hline
    n  &  $x_n$ \\
    \hline
    \hline
        1 & 0.8414709848078965 \\
  \hline
       10 & 0.48132935526234627 \\
  \hline
      100 & 0.1696653247073242  \\
  \hline
     1000 & 0.05462012602579727   \\
  \hline
    10000 & 0.017314486231827124 \\
  \hline
   100000 & 0.005476997236720512 \\
  \hline
  1000000 &  0.0017320423900648602 \\
  \hline
 10000000 & 5.477222534834344E-4  \\
  \hline 
100000000 & 1.7320506994644328E-4 \\
  \hline
  \end{tabular}
  \caption{Werte der durch $x_{n+1} = \sin(x_n)$ definierten Folge $\folge{x_n}$.}
  \label{tab:sinXisX}
\end{table}




\subsection{Beschleunigung der Fixpunkt-Iteration}
Auch mit 36 Iterationen ist die Fixpunkt-Iteration bei der L\"osung der Gleichung $x = \cos(x)$
 dem Bisektions-Verfahren unterlegen.
Wir stellen uns daher die Frage, ob wir die Konvergenz der Fixpunkt-Iteration
beschleunigen k\"onnen.  Falls die kontrahierende Abbildung $f$ differenzierbar ist, so ist der
Kontraktions-Koeffizient durch den Betrag der Ableitung von $f$ gegeben.  Wir \"uberlegen uns daher,
wie wir die Abbildung so ver\"andern k\"onnen, dass sich einerseits der Fixpunkt nicht \"andert,
aber andererseits der Betrag der Ableitung kleiner wird.  Dazu formen wir die
Fixpunkt-Gleichung $x = f(x)$ wie folgt um:
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{crcll}
                & x & = & f(x)                                           & \mid\; + \;\alpha \cdot  x            \\[0.2cm]
\Leftrightarrow & \ds (1 + \alpha) \cdot  x & = & f(x) + \alpha \cdot  x & \ds\mid\; \cdot  \;\frac{1}{1 + \alpha} \\[0.2cm]
\Leftrightarrow &                x & = & \ds\frac{f(x) + \alpha \cdot  x}{1 + \alpha}            
\end{array}
$
\\[0.2cm]
Damit haben wir also die Funktion 
\\[0.2cm]
\hspace*{1.3cm}
$g(x) = \ds\frac{f(x) + \alpha \cdot  x}{1 + \alpha}$
\\[0.2cm]
gefunden, welche dieselben Fixpunkte hat wie die urspr\"ungliche Funktion $f$.  F\"ur die
Ableitung gilt 
\\[0.2cm]
\hspace*{1.3cm}
$\ds g'(x) = \frac{f'(x) + \alpha}{1 + \alpha}$
\\[0.2cm]
Der Betrag diese Ableitung wird f\"ur den Fixpunkt $\bar{x}$ dann am kleinsten, wenn wir 
\\[0.2cm]
\hspace*{1.3cm}
$\alpha = - f'(\bar{x})$ 
\\[0.2cm]
w\"ahlen.  W\"ahlen wir beispielsweise $\alpha = 0.7$, so finden wir die L\"osung der
Fixpunkt-Gleichung $x = \cos(x)$ mit dem Programm in Abbildung \ref{fig:cosXisX.stlx}
die in Tabelle \ref{tab:alpha-x-cos-x} gezeigten Werte.  Wir sehen, dass bereits nach f\"unf
Iterations-Schritten eine Genauigkeit von mehr als $10^{-6}$ erreicht ist.

\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 1.3cm,
                  xrightmargin  = 1.3cm,
                ]
    x     := 0;
    alpha := read("input alpha");
    for (i in [1 .. 12]) {
        x := 1 / (1 + alpha) * (cos(x) + alpha * x);
        print("$i$: $x$");
    }
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{Berechnung der durch $\ds x_{n+1} = \frac{\cos(x) + \alpha \cdot  x_n}{1 + \alpha}$ definierten
    Folge.}
  \label{fig:cosXisX.stlx}
\end{figure} %\$


\begin{table}[!h]
  \centering
\framebox{
  \begin{tabular}{|l|c|l|c|}
\hline
   $n$ & $x_n$ & $n$ & $x_n$  \\
\hline
1  & 0.588235294117647 & 7  & 0.739085133207671 \\
\hline
2  & 0.731579943641669 & 8  & 0.739085133215044 \\
\hline
3  & 0.738956362842702 & 9  & 0.739085133215159 \\
\hline
4  & 0.739083130793540 & 10 & 0.739085133215161 \\
\hline
5  & 0.739085102132028 & 11 & 0.739085133215161 \\
\hline
6  & 0.739085132732678 & 12 & 0.739085133215161 \\
\hline
  \end{tabular}}
  \caption{Die ersten 12 Glieder der durch $x_{n+1} = \frac{\cos(x) + \alpha \cdot  x_n}{1 + \alpha}$ definierten
    Folge f\"ur $\alpha = 0.7$.}
  \label{tab:alpha-x-cos-x}
\end{table}

Das oben skizzierte Verfahren der Konvergenz-Beschleunigung hat einen Sch\"onheitsfehler:
Wir haben $\alpha$  so gew\"ahlt, dass $f'(\bar{x}) + \alpha$ m\"oglichst klein
wird.  Das Problem dabei ist, dass wir $\bar{x}$ gar nicht kennen und daher auch
$f'(\bar{x})$ unbekannt ist.  Eine m\"ogliche L\"osung besteht darin, dass wir f\"ur $\alpha$
in jedem Schritt $-f'(x_n)$ einsetzen.  Das f\"uhrt auf folgende
Definition f\"ur die Folge $\folge{x_n}$:
\begin{equation}
  \label{eq:fixpunktNewton}
   x_{n+1} = \frac{f(x_n) - f'(x_n) \cdot  x_n}{1 - f'(x_n)}  
\end{equation}
Abbildung \ref{fig:cosXisX-newton.stlx} zeigt ein Programm zur Umsetzung dieser Idee.
Hier haben wir die ersten 7 Werte mit Gleichung (\ref{eq:fixpunktNewton})
berechnet, die f\"ur den Fall $f(x) = \cos(x)$ die Form
\\[0.2cm]
\hspace*{1.3cm}
$\ds x_{n+1} := \frac{cos(x) + \sin(x) \cdot x}{1 + \sin(x)}$
\\[0.2cm]
annimmt.  Da diese Gleichung aber komplexer als die urspr\"ungliche Gleichung
$x_{n+1} = \cos(x_n)$ ist, m\"ussen wir damit rechnen, dass die Rundungfehler h\"oher sind als bei der
Iteration.  Zur  Eliminierung dieser Rundungfehler f\"uhren wir daher in Zeile 8
noch eine Nach-Iteration mit der Gleichung $x_{n+1} = \cos(x_n)$ durch.
Tabelle \ref{tab:solution-smart} zeigt die von diesem Programm berechneten Werte.  Diesmal
ist die Genauigkeit von $10^{-6}$ bereits nach 4 Schritten erreicht, de facto ist der im
vierten Schritt berechnete Wert sogar auf 9 Stellen hinter dem Komma genau.


\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 1.3cm,
                  xrightmargin  = 1.3cm,
                ]
    x := 0; 
    n := 7;
    for (i in [1 .. 7]) {
        alpha := -sin(x);
        x := (cos(x) - alpha * x) / (1 - alpha);
        print("$i$: $x$");
    }
    x := cos(x);
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{Berechnung der durch $x_{n+1} = \frac{\cos(x_n) + \sin(x_n) \cdot  x}{1 + \sin(x_n)}$ definierten Folge.}
  \label{fig:cosXisX-newton.stlx}
\end{figure} %\$

\begin{table}[!h]
  \centering
\framebox{
  \begin{tabular}{|l|l|}
\hline
   $n$ & $x_n$  \\
\hline
\hline
$1$ & 1.0                \\
\hline
$2$ & 0.7503638678402439 \\
\hline
$3$ & 0.7391128909113617 \\
\hline
$4$ & 0.7390851333852839 \\
\hline
$5$ & 0.7390851332151607 \\
\hline
$6$ & 0.7390851332151607 \\
\hline
  \end{tabular}}
  \caption{Ausgabe des in Abbildung \ref{fig:cosXisX-newton.stlx} gezeigten Programms.}
  \label{tab:solution-smart}
\end{table}

\subsection{Das Newton'sche Verfahren zur Berechnung von Nullstellen}
Oft besteht die Aufgabe darin, eine Nullstelle einer Funktion $g(x)$ zu finden.
Dieses Problem ist dazu \"aquivalent, eine Fixpunkt-Gleichung zu l\"osen, denn es gilt
\\[0.2cm]
\hspace*{1.3cm}
$g(x) = 0 \;\Leftrightarrow\; x + g(x) = x$.
\\[0.2cm]
Eine Nullstelle der Funktion $g(x)$ ist also ein Fixpunkt der Funktion $f(x) = x + g(x)$.
Setzen wir in Gleichung (\ref{eq:fixpunktNewton}) f\"ur $f(x)$ die Funktion $x + g(x)$ ein,
so erhalten wir wegen
\\[0.2cm]
\hspace*{1.3cm}
$\dfo \bigl(x+g(x)\bigr) = 1 + g'(x)$
\\[0.2cm]
die Gleichung 
\\[0.2cm]
\hspace*{1.3cm}
$\begin{array}[t]{lcl}
 x_{n+1} & = & \ds\frac{x_n + g(x_n) - \bigl(1 + g'(x_n)) \cdot  x_{n}}{1 - \bigl(1 + g'(x_n)\bigr)} \\[0.5cm]
        & = & \ds\frac{x_n + g(x_n)  - x_{n} - g'(x_n)\cdot x_n}{-g'(x_n)} \\[0.5cm]
        & = & \ds\frac{- g(x_n) + g'(x_n)\cdot x_n}{g'(x_n)} \\[0.5cm]
        & = & \ds x_n - \frac{g(x_n)}{g'(x_n)} \\[0.5cm]
 \end{array}
$
\\[0.2cm]
Wir haben also  die Iterations-Vorschrift 
\begin{equation}
  \label{eq:NewtonZero}
  \colorbox{red}{\colorbox{orange}{\framebox{$\ds x_{n+1} = x_n - \frac{g(x_n)}{g'(x_n)}$}}}
\end{equation}
gefunden.  Dieses Verfahren wird als  \emph{Newton'sches Verfahren} 
(\href{https://en.wikipedia.org/wiki/Isaac_Newton}{Sir Isaac Newton}, 1642 -- 1726)
bezeichnet.
Die Gleichung  (\ref{eq:NewtonZero}) l\"asst sich geometrisch interpretieren: Legen wir im Punkt
$\langle x_n, g(x_n) \rangle$ eine Tangente an die Funktion $g$, so schneidet diese Tangente die
$x$-Achse im Punkt 
\\[0.2cm]
\hspace*{1.3cm}
$\ds x_n - \frac{g(x_n)}{g'(x_n)}$.
\\[0.2cm]
Der neue Wert $x_{n+1}$ ist also die N\"aherung, die wir erhalten, wenn wir die Funktion $g$ durch die
Tangente im Punkt $\langle x_n, g(x_n) \rangle$ ersetzen und dann die Nullstelle  dieser
Tangente berechnen.

\exercise
Beweisen Sie diese Behauptung. \eox


\example
Als Anwendung des
Newton'schen Verfahrens betrachten wir die Berechnung der $k$-ten Wurzel ($k\in\mathbb{N}$ mit
$k\geq 2$) einer gegebenen Zahl $a>0$. Wegen 
\\[0.2cm]
\hspace*{1.3cm}
$\ds x = \sqrt[k]{a} \quad\Leftrightarrow\quad x^k - a = 0$
\\[0.2cm]
setzen wir $g(x) := x^k - a$ und bestimmen die Nullstellen der Funktion $g(x)$ mit dem
Newton'schen Verfahren.  F\"ur die Ableitung der Funktion $g(x)$ finden wir 
\\[0.2cm]
\hspace*{1.3cm}
$g'(x) = k\cdot x^{k-1}$.
\\[0.2cm]
Damit lautet die Iterations-Vorschrift
\\[0.3cm]
\hspace*{1.3cm}
$\ds x_{n+1} = x_n - \frac{x_n^k - a}{k\cdot x_n^{k-1}} = \frac{1}{k} \left( (k-1)\cdot x_n + \frac{a}{x_n^{k-1}}\right)$.
\\[0.3cm]
Berechnen wir mit diesem Verfahren die dritte Wurzel aus $2$, so lautet die Iterations-Vorschrift
\\[0.2cm]
\hspace*{1.3cm}
$\ds x_{n+1} = \frac{1}{3} \cdot \left( 2\cdot x_n + \frac{2}{x_n^{2}}\right)$
\\[0.2cm]                                 
Lassen wir die Folge mit 1 starten so finden wir die Werte 
\\[0.2cm]
\hspace*{1.3cm}
$1.0$, $1.333333333$, $1.263888889$, $1.259933494$, $1.259921050$ 
\\[0.2cm]
und der letzte Wert stimmt im Rahmen der Rechengenauigkeit mit $\sqrt[3]{2}$ \"uberein.

Das Newton'sche Verfahren ist \underline{nicht} robust, denn im Allgemeinen  konvergiert das Verfahren
nicht.  Als Beispiel betrachten wir die 
Funktion 
\\[0.2cm]
\hspace*{1.3cm}
$g(x) = x^3 - 2 \cdot x + 2$.
\\[0.2cm]
Es gilt $g(-2) = -8 - 2 \cdot (-2) + 2 = -2 < 0$ und $g(2) = 8 - 2 \cdot 2 + 2 = 6 > 0$.  Nach dem 
Zwischenwert-Satz \"uber stetige Funktionen muss die Funktion $g$ daher in dem Intervall $[-2, 2]$ eine
Nullstelle haben.  Das Newton'sche Verfahren ergibt die Formel
\\[0.2cm]
\hspace*{1.3cm}
$\ds x_{n+1} = x_n - \frac{x_n^3 - 2 \cdot x_n + 2}{3 \cdot x_n^2 - 2}$.
\\[0.2cm]
W\"ahlen wir als Start-Wert $x_0 := 0$, so erhalten wir 
\\[0.2cm]
\hspace*{1.3cm}
$\ds x_1 = 0 - \frac{2}{-2} = 1$.
\\[0.2cm]
F\"ur $x_2$ finden wir
\\[0.2cm]
\hspace*{1.3cm}
$\ds x_{2} = 1 - \frac{1 - 2 + 2}{3 - 2} = 1 - 1 = 0 = x_0$.
\\[0.2cm]
Wir sind also wieder bei unserem Start-Wert angekommen!  Damit gilt allgemein
\\[0.2cm]
\hspace*{1.3cm}
$x_n = \left\{
\begin{array}{ll}
  0 & \mbox{falls $n \,\texttt{\%}\, 2 = 0$,} \\[0.2cm]
  1 & \mbox{falls $n \,\texttt{\%}\, 2 = 1$.}
\end{array} 
\right.
$
\\[0.2cm] %$
Folglich konvergiert das Verfahren in diesem Fall nicht.  W\"ahlen wir den Startwert $x_0 = -0.5$, so
oszilliert die Folge $\folge{x_n}$ f\"ur gro{\ss}e $n$ ebenfalls zwischen den Werten $0$ und $1$.
W\"ahlen wir den Startwert nahe genug zur gesuchten L\"osung, beispielsweise $x_0 = -1.5$, so
konvergiert das Verfahren gegen die L\"osung 
$-1.769292354238631\cdots$.  Diese Beispiele zeigen, dass das Newton'sche Verfahren ohne weitere
Einschr\"ankungen nicht zuverl\"assig ist.

\subsection{Analyse des Newton'schen Verfahrens}
Wir wollen in diesem Abschnitt herausfinden, unter welchen Umst\"anden das Newton'sche Verfahren
konvergiert und wollen au{\ss}erdem die Geschwindigkeit der Konvergenz des Verfahrens untersuchen.
Als erstes ben\"otigen wir einen Hilfssatz \"uber konvexe Funktionen.

\begin{Lemma}
  Die Funktion $f:[a,b] \rightarrow \mathbb{R}$ sei zweimal differenzierbar und konvex.
  Ist $x_0 \in [a,b]$ und definieren wir die lineare Funktion $g:[a,b] \rightarrow \mathbb{R}$ als
  \\[0.2cm]
  \hspace*{1.3cm}
  $g(x) := f(x_0) + f'(x_0) \cdot (x - x_0)$,
  \\[0.2cm]
  so ist die Funktion $g$ die Tangente an $f$ im Punkt $\langle x_0, f(x_0) \rangle$ und es gilt 
  \\[0.2cm]
  \hspace*{1.3cm}
  $g(x) \leq f(x)$ \quad f\"ur alle $x \in [a,b]$.
  \\[0.2cm]
  Anschaulich bedeutet dies, dass die Tangente immer unterhalb einer konvexen Funktion liegt.
\end{Lemma}

\proof
Zun\"achst gilt offenbar
\\[0.2cm]
\hspace*{1.3cm}
$g(x_0) = f(x_0) + f'(x_0) \cdot (x_0 - x_0) = f(x_0)$,
\\[0.2cm]
so dass die Werte der Funktionen $f$ und $g$ im Punkt $x_0$ \"ubereinstimmen.  F\"ur die Ableitung von
$g(x)$ finden wir
\\[0.2cm]
\hspace*{1.3cm}
$g'(x) = f'(x_0)$,
\\[0.2cm]
so dass die Gerade $g$ dieselbe Steigung hat wie die Funktion $f$ an der Stelle $x_0$.  Damit ist $g$
aber die Tangente an die Funktion $f$ an der Stelle $x_0$.
Zum Beweis der Ungleichung $g(x) \leq f(x)$ definieren wir die Funktion $h:[a-x_0, b-x_0] \rightarrow \mathbb{R}$ als  
\\[0.2cm]
\hspace*{1.3cm}
$h(x) := f(x + x_0)$.
\\[0.2cm]
Brechen wir die Taylor-Entwicklung der Funktion $h$ nach dem linearen Glied ab, so erhalten wir nach
der Formel (\ref{eq:taylorLagrange}) die Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$h(x) = h(0) + h'(0) \cdot x  + \frac{1}{2} \cdot h^{(2)}(\chi) \cdot x^{2}$, 
\\[0.2cm]
wobei $\chi$ ein Element des Intervalls $[a-x_0, b-x_0]$ ist, \"uber das wir sonst nichts wissen.
Aufgrund der Gleichungen
\\[0.2cm]
\hspace*{1.3cm}
 $f(x) = h(x - x_0)$, \quad $h(0) = f(x_0)$, \quad $h'(0) = f'(x_0)$ 
\quad und \quad $h^{(2)}(\chi) =  f^{(2)}(\chi + x_0)$,
\\[0.2cm]
folgt daraus f\"ur die Funktion $f$ die Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$f(x) = f(x_0) + f'(x_0) \cdot (x - x_0)  + \frac{1}{2} \cdot f^{(2)}(\chi + x_0) \cdot (x- x_0)^{2}$.
\\[0.2cm]
Definieren wir $\varphi := \chi + x_0$, so gilt $\varphi \in [a,b]$ und wir haben
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[t]{lcl}
f(x) &   =  & \ds f(x_0) + f'(x_0) \cdot (x - x_0) + \frac{1}{2} \cdot f^{(2)}(\varphi) \cdot (x-x_0)^{2} \\[0.2cm]
     &   =  & \ds g(x) + \frac{1}{2} \cdot f^{(2)}(\varphi) \cdot (x-x_0)^{2}                             \\[0.2cm]
     & \geq & g(x),                                    
\end{array}
$
\\[0.2cm] 
denn da wir angenommen hatten, dass die Funktion $f$ 
zweimal differenzierbar und konvex ist, gilt
\\[0.2cm]
\hspace*{1.3cm}
$f^{(2)}(\varphi) \geq 0$ 
\\[0.2cm]
und das Quadrat $(x-x_0)^{2}$ ist sicher immer gr\"o{\ss}er oder gleich $0$.  \qed

\begin{Satz}[Konvergenz des Newtonschen Verfahrens f\"ur monotone und konvexe Funktionen] \lb
  Die Funktion $f:[a,b] \rightarrow \mathbb{R}$ sei zweimal differenzierbar und konvex, es gelte
  $f(a) < 0$, $f(b) > 0$ und 
  \\[0.2cm]
  \hspace*{1.3cm}
  $\forall x \in [a,b]: f'(x) > 0$.
  \\[0.2cm]
  Falls der Startwert $x_0 \in [a,b]$ so gew\"ahlt wird, dass $f(x_0) > 0$ ist, 
  dann ist die durch das Newton'sche Verfahren definierte Folge $\folge{x_n}$ monoton fallend und
  beschr\"ankt und damit konvergent.  F\"ur den Grenzwert
  $\bar{x} := \lim\limits_{n\rightarrow\infty} x_n$
  gilt $f(\bar{x}) = 0$.
\end{Satz}

\proof
Nach dem Zwischenwert-Satz hat die Funktion eine Nullstelle $\xi$ in dem Intervall $[a,b]$.  Da 
$f'(x)$ f\"ur alle $x \in [a,b]$ echt gr\"o{\ss}er als Null ist, kann $f$ keine zweite Nullstelle haben, denn
sonst h\"atten wir einen Widerspruch zum Satz von Rolle.  Also gilt
\\[0.2cm]
\hspace*{1.3cm}
$\forall x \in [a,\xi): f(x) < 0$ \quad und \quad
$\forall x \in (\xi,b]: f(x) > 0$.
\\[0.2cm]
Wir zeigen zun\"achst durch Induktion \"uber $n$, dass 
\\[0.2cm]
\hspace*{1.3cm}
$f(x_n) \geq 0$ \quad f\"ur alle $n \in \mathbb{N}_0$ gilt.  
\begin{enumerate}
\item[I.A.] $n=0$.  

            Die Ungleichung $f(x_0) > 0$  gilt nach Voraussetzung. $\surd$
\item[I.S.] $n \mapsto n+1$.

            Nach der Definition ist $x_{n+1}$ die Nullstelle der Tangente $g$ an die Funktion $f$
            im Punkt $x_n$, es gilt also $g(x_{n+1}) = 0$.  Nach dem letzten Lemma gilt $g(x) \leq f(x)$.  
            Durch Einsetzen von $x_{n+1}$ folgt daraus die Ungleichung
            \\[0.2cm]
            \hspace*{1.3cm}
            $0 \leq f(x_{n+1})$.
            \\[0.2cm]
            Damit ist die Induktion abgeschlossen. $\surd$
\end{enumerate}
Nach Definition von $x_{n+1}$ gilt
\\[0.2cm]
\hspace*{1.3cm}
$\ds x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}$.
\\[0.2cm]
Einerseits haben wir gerade gezeigt, dass $f(x_n) \geq 0$ ist, andererseits ist die Voraussetzung, dass
f\"ur alle $x \in [a,b]$ die Ungleichung $f'(x) > 0$ gilt.  Daraus folgt aber
\\[0.2cm]
\hspace*{1.3cm}
$\ds\frac{f(x_n)}{f'(x_n)} \geq 0$
\\[0.2cm]
und damit gilt
\\[0.2cm]
\hspace*{1.3cm}
$x_{n+1} \leq x_n$.
\\[0.2cm]
Dies zeigt, dass die Folge $\folge{x_n}$ monoton fallend ist.  Da wir andererseits wissen, dass
$f(x_n) \geq 0$ ist, muss $x_n \geq \xi$ gelten.  Also ist die Folge $x_n$ durch $\xi$ nach unten beschr\"ankt.
Als monoton fallend und beschr\"ankte Folge hat $\folge{x_n}$ damit einen Grenzwert $\bar{x}$.  
F\"ur diesen Grenzwert gilt 
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}{lcl}
  \bar{x} & = & \ds\lim\limits_{n\rightarrow\infty} x_n                             \\[0.3cm]
          & = & \ds\lim\limits_{n\rightarrow\infty} x_{n+1}                         \\[0.3cm]
          & = & \ds\lim\limits_{n\rightarrow\infty} x_{n} - \frac{f(x_n)}{f'(x_n)} \\[0.3cm]
          & = & \ds\bar{x} - \frac{f(\bar{x})}{f'(\bar{x})}. 
\end{array}
$
\\[0.2cm]
Aus der Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$\ds \bar{x} = \bar{x} - \frac{f(\bar{x})}{f'(\bar{x})}$ \quad folgt sofort \quad $\ds 0 = - \frac{f(\bar{x})}{f'(\bar{x})}$
\\[0.2cm]
und daraus folgt durch Multiplikation mit $-f'(\bar{x})$ die gesuchte Gleichung $f(\bar{x}) = 0$.  \qed

\subsubsection{Analyse der Konvergenz-Geschwindigkeit}
Benutzen wir das Newton'sche Verfahren zur Berechnung  von $\sqrt{2\,}$, so erhalten wir 
die folgenden Ergebnisse:
{\footnotesize
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = none,
                  numbersep     = -0.2cm,
                  xleftmargin   = -0.6cm,
                  xrightmargin  = -0.6cm,
                ]
x0 = 2
x1 = 1.50000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
x2 = 1.41666666666666666666666666666666666666666666666666666666666666666666666666666666666666666
x3 = 1.41421568627450980392156862745098039215686274509803921568627450980392156862745098039215686
x4 = 1.41421356237468991062629557889013491011655962211574404458490501920005437183538926835899004
x5 = 1.41421356237309504880168962350253024361498192577619742849828949862319582422892362178494183
x6 = 1.41421356237309504880168872420969807856967187537723400156101313311326525563033997853178716
x7 = 1.41421356237309504880168872420969807856967187537694807317667973799073247846210703885038753
x8 = 1.41421356237309504880168872420969807856967187537694807317667973799073247846210703885038753
x9 = 1.41421356237309504880168872420969807856967187537694807317667973799073247846210703885038753
\end{Verbatim}
}
Wir sehen, dass $x_2$ auf 2 Stellen hinter dem Komma mit $\sqrt{2\,}$ \"ubereinstimmt,
bei $x_3$ sind bereits 5 Stellen richtig, $x_4$ hat eine Genauigkeit von 11 Stellen,
$x_5$ stimmt auf 24 Stellen mit $\sqrt{2\,}$ \"uberein, bei $x_6$ sind es 48 Stellen und $x_7$ hat
bereits eine Genauigkeit von 100 Stellen.  Wir beobachten, dass sich die Zahl der korrekten Stellen mit
jeder Operation etwa verdoppelt.  Diese Ph\"anomen wollen wir nun genauer untersuchen.
Dazu entwicken wir die Funktion $f(x)$ an der Stelle $x_n$  in einer Taylor-Reihe, die wir nach dem
linearen Glied abbrechen.  Wir erhalten
\\[0.2cm]
\hspace*{1.3cm}
$\ds f(x) = f(x_n) + f'(x_n) \cdot (x- x_n) + \frac{1}{2} \cdot f^{(2)}(\varphi) \cdot (x - x_n)^2$
\\[0.2cm]
Hier setzen wir f\"ur $x$ den Wert $\bar{x}$, also die Nullstelle von $f$ ein und erhalten
\\[0.2cm]
\hspace*{1.3cm}
$\ds 0 =  f(x_n) + f'(x_n) \cdot (\bar{x} - x_n) + \frac{1}{2} \cdot f^{(2)}(\varphi) \cdot (\bar{x} - x_n)^2$
\\[0.2cm]
Wir subtrahieren $f(x_n)$ und teilen anschlie{\ss}end durch $f'(x_n)$.  Das liefert die Gleichung
\\[0.2cm]
\hspace*{1.3cm}
$\ds - \frac{f(x_n)}{f'(x_n)} = 
 \bar{x} - x_n + \frac{1}{2} \cdot \frac{f^{(2)}(\varphi)}{f'(x_n)} \cdot (\bar{x} - x_n)^2$
\\[0.2cm]
Jetzt addieren wir $x_n$ und subtrahieren $\bar{x}$.  Das liefert
\\[0.2cm]
\hspace*{1.3cm}
$\ds x_n - \frac{f(x_n)}{f'(x_n)} - \bar{x} = 
 \frac{1}{2} \cdot \frac{f^{(2)}(\varphi)}{f'(x_n)} \cdot (\bar{x} - x_n)^2
$.
\\[0.2cm]
An dieser Stelle bemerken wir, dass $\ds x_n - \frac{f(x_n)}{f'(x_n)} = x_{n+1}$ ist und haben damit
\\[0.2cm]
\hspace*{1.3cm}
$\ds x_{n+1} - \bar{x} = \frac{1}{2} \cdot \frac{f^{(2)}(\varphi)}{f'(x_n)} \cdot (\bar{x} - x_n)^2$.
\\[0.2cm]
Wir definieren nun $\varepsilon_n := |x_n - \bar{x}|$.  Der Wert $\varepsilon_n$ gibt also den
Betrag des Approximations-Fehler an, den wir nach der $n$-ten Iteration des Newton'schen Verfahrens
haben.  Damit schreibt sich die letzte Gleichung als
\\[0.2cm]
\hspace*{1.3cm}
$\ds \varepsilon_{n+1} = \frac{1}{2} \cdot \left|\frac{f^{(2)}(\varphi)}{f'(x_n)}\right| \cdot \varepsilon_n^2$.
\\[0.2cm]
In den F\"allen, in denen  wir den Ausdruck 
$\ds \frac{1}{2} \cdot \left|\frac{f^{(2)}(\varphi)}{f'(x_n)}\right|$ 
durch
eine Konstante $K$ absch\"atzen k\"onnen, haben wir dann
\\[0.2cm]
\hspace*{1.3cm}
$\varepsilon_{n+1} \leq  K \cdot \varepsilon_{n}^2$.
\\[0.2cm]
Die Zahl der korrekten Stellen nach der $n$-ten Iteration ist in etwa durch
\\[0.2cm]
\hspace*{1.3cm}
$\lambda_{n} \approx -\log_{10}(\varepsilon_n)$ 
\\[0.2cm]
gegeben.  Logarithmieren wir die obige Gleichung zur Basis 10 und multiplizieren mit $-1$, so erhalten wir
\\[0.2cm]
\hspace*{1.3cm}
$\lambda_{n+1} \geq 2 \cdot \lambda_n - \log_{10}(K)$.
\\[0.2cm]
Zur Konkretisierung unserer \"Uberlegungen betrachten wir wieder die Funktion $f(x) = x^2 -2$.  Hier gilt
\\[0.2cm]
\hspace*{1.3cm}
$f'(x) = 2 \cdot x \geq 2$ \quad falls $x \geq 1$ ist
\\[0.2cm]
und weiter gilt $f^{(2)}(x) = 2$.  Damit gilt
\\[0.2cm]
\hspace*{1.3cm}
$\ds \frac{1}{2} \cdot \left|\frac{f^{(2)}(\varphi)}{f'(x_n)}\right| \leq \frac{1}{2}$
\\[0.2cm]
und wegen $\log_{10}\bigl(\frac{1}{2}\bigr) \approx - 0.3$ haben wir die Absch\"atzung
\\[0.2cm]
\hspace*{1.3cm}
$\lambda_{n+1} = 2 \cdot \lambda_{n} + 0.3$ 
\\[0.2cm]
gefunden, die in der Tat zeigt, dass sich die Anzahl der korrekten Stellen bei jedem Schritt mehr als verdoppelt.
\pagebreak

\exercise
Analysieren Sie das Newton'sche Verfahren zur Berechnung der dritten Wurzel aus $2$ und berechnen Sie,
wieviele Iterationen h\"ochstens notwendig sind um den Wert von $\sqrt[3]{2}$ auf eine Genauigkeit von
$10^{-101}$ zu berechnen.
\eox




\section{Iterative L\"osung linearer Gleichungs-Systeme$^*$}
Es sei eine $n\times n$ Matrix $\mathbf{A} \in \mathbb{R}^{n\times n}$ und ein Vektor
 $\vec{b}\in \mathbb{R}^n$ gegeben.  Eine M\"oglichkeit, 
das Glei\-chungs-System 
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{A}\, \vec{x} = \vec{b}$
\\[0.2cm]
zu l\"osen, haben Sie im ersten Semester kennegelernt: Es ist das Gau{\ss}'sche
Eliminations-Verfahren (\href{https://de.wikipedia.org/wiki/Carl_Friedrich_Gauss}{Carl Friedrich Gau{\ss}}, 1777 -- 1855).  
Es gibt allerdings Situationen, in denen dieses
Verfahren zu 
aufwendig ist.  Dies ist beispielsweise dann der Fall, wenn einerseits $n$ gro{\ss} ist und
wenn andererseits die meisten Komponenten der Matrix $A$ den Wert 0 haben.  Solche
Matrizen hei{\ss}en 
\href{https://de.wikipedia.org/w/index.php?title=Schwachbesetzte_Matrix&redirect=yes}{\emph{d\"unn
    besetzte} Matrizen}.  Diese Art von Matrizen tritt beispielsweise bei  
der numerischen L\"osung von
\href{http://de.wikipedia.org/wiki/Partielle_Differentialgleichung}{partiellen Differential-Gleichungen} auf.   
Das Gau{\ss}'sche Eliminations-Verfahren besitzt  eine
Komplexit\"at von $O(n^3)$ und ist damit f\"ur gro{\ss}e d\"unn besetzte Matrizen ungeeignet.
Hier ist der Rechenaufwand bei iterative Verfahren geringer.   
Ein weiteres Problem ist, dass die Rundungsfehler beim Gau{\ss}'schen Eliminations-Verfahren
sehr gro{\ss} werden k\"onnen.  Demgegen\"uber sind iterative Verfahren \emph{selbstkorrigierend}.

Ist $\mathbf{A}$ gegeben, so lautet die Gleichung $\mathbf{A}\,\vec{x} = \vec{b}$ in Komponenten-Schreibweise 
\\[0.2cm]
\hspace*{1.3cm}
$\displaystyle \sum\limits_{j=1}^n a_{ij} \cdot  x_j = b_i$ \quad f\"ur alle $i = 1,\cdots,n$.
\\[0.2cm]
Die Idee besteht darin, diese Gleichung in eine Fixpunkt-Gleichung zu transformieren.
Dazu formen wir die Gleichung wie folgt um: 
\\[0.3cm]
\hspace*{1.3cm}
$
\begin{array}{crcl}
                &  \displaystyle \sum\limits_{j=1}^n a_{ij} \cdot  x_j & = & b_i \\[0.6cm]
\Leftrightarrow & \displaystyle a_{ii} \cdot  x_i + \sum\limits_{j=1 \atop j \not= i}^n a_{ij} \cdot  x_j & = & b_i \\[0.2cm]
\Leftrightarrow & a_{ii} \cdot  x_i & = & \displaystyle b_i - \sum\limits_{j=1 \atop j \not= i}^n a_{ij} \cdot  x_j \\[0.6cm]
\Leftrightarrow & x_i & = & \displaystyle \frac{1}{a_{ii}} \cdot  \Biggl( b_i - \sum\limits_{j=1 \atop j \not= i}^n a_{ij} \cdot  x_j \Biggr). \\[0.3cm]
\end{array}
$
\\[0.2cm]
Diese Umformung liefert uns die Iterations-Vorschrift
\\[0.2cm]
\hspace*{1.3cm}
$x^{(n+1)}_i = \displaystyle \frac{1}{a_{ii}} \cdot  \Biggl( b_i - \sum\limits_{j=1 \atop j \not= i}^n a_{ij} \cdot  x^{(n)}_j \Biggr)$
\\[0.2cm]
mit der wir versuchen k\"onnen, eine L\"osung der Fixpunkt-Gleichung zu finden.
Das Verfahren, das wir auf diese Weise erhalten, wird als \emph{Gesamtschritt-Verfahren}
oder auch als \href{https://de.wikipedia.org/wiki/Jacobi-Verfahren}{\emph{Jacobi-Verfahren}} 
(\href{https://de.wikipedia.org/wiki/Carl_Gustav_Jacob_Jacobi}{Carl Gustav Jacob Jacobi}, 1804 -- 1851) bezeichnet.
Die Frage lautet jetzt, wann die durch dieses Verfahren definierte Iteration konvergiert.
Um eine positive Antwort auf diese Frage geben zu k\"onnen, ben\"otigen wir die folgenden Definitionen.

\begin{Definition}[Starkes Zeilen-Summen-Kriterium]
  Eine $n\times n$ Matrix $A \in \mathbb{R}^{n\times n}$ erf\"ullt das \emph{starke Zeilen-Summen-Kriterium} 
 falls es eine Zahl $q < 1$ gibt, so dass gilt
  \\[0.2cm]
  \hspace*{1.3cm}
  $\displaystyle\sum\limits_{j=1 \atop j \not= i}^n \bigl| a_{ij} \bigr| \leq q \cdot
  \bigl| a_{ii} \bigr|$ \quad f\"ur alle $i = 1,\cdots,n$.
\end{Definition}

\noindent
Falls $a_{ii} \not=0$ ist, ist die in der obigen Definition gegebene Ungleichung 
\"aquivalent zu  der Ungleichung
\\[0.2cm]
\hspace*{1.3cm}
$\displaystyle\frac{1}{\bigl|a_{ii}\bigr|}\cdot\sum\limits_{j=1 \atop j \not= i}^n \bigl| a_{ij} \bigr| \leq q$. 


\noindent
Als n\"achstes f\"uhren wir ein Konzept ein, das den Begriff des Betrags auf Vektoren aus
dem $\mathbb{R}^n$ verallgemeinert.
\begin{Definition}[Maximums-Norm]
  Ist $\vec{x} \in \mathbb{R}^{n}$, so definieren wir die Maximums-Norm von
  $\vec{x}$ als 
  \\[0.2cm]
  \hspace*{1.3cm} $\norm{\vec{x}} := \max\bigl\{\, |x_i| \mid i \in \{1,\cdots,n\}\, \bigr\}$.
\end{Definition}
Mit der so definierten Norm k\"onnen wir so rechnen, wie wir das von Betr\"agen bei reellen
Zahlen gew\"ohnt sind,  insbesondere gilt auch die \emph{Dreiecks-Ungleichung}, wir haben also
f\"ur beliebige Vektoren $\vec{x}, \vec{y} \in \mathbb{R}^{n}$ 
\\[0.3cm]
\hspace*{1.3cm}
$\norm{\vec{x} + \vec{y}} \leq \norm{\vec{x}} + \norm{\vec{y}}$.
\\[0.3cm]
Au{\ss}erdem haben wir f\"ur reelle Zahlen $\alpha \in\mathbb{R}$ und Vektoren
$\vec{x}\in\mathbb{R}^n$ die Gleichung 
\\[0.2cm]
\hspace*{1.3cm}
$\norm{ \alpha\,\vec{x} } = |\alpha|\cdot  \norm{\vec{x}}$
\\[0.2cm]
Hierbei bezeichnet $\alpha\,\vec{x}$ die komponentenweise Multiplikation des Vektors
$\vec{x}$ mit der Zahl $\alpha$.
Schlie{\ss}lich gilt f\"ur alle $\vec{x}\in\mathbb{R}^n$ die Ungleichung 
$0 \leq \norm{\vec{x}}$ wobei Gleichheit nur im Fall $\vec{x} = \vec{0}$ auftritt:
\\[0.2cm]
\hspace*{1.3cm}
$\norm{\vec{x}} = 0 \;\Rightarrow\; \vec{x} = \vec{0}$ \quad f\"ur alle $\vec{x}\in\mathbb{R}$.
\\[0.2cm] 
Der folgende Satz beantwortet nun die oben gestellte Frage nach der Konvergenz des
Gesamtschritt-Verfahrens in einem f\"ur Anwendungen wichtigen Spezial-Fall.

\begin{Satz}
  Wenn die Matrix  $A \in \mathbb{R}^{n\times n}$  das starke
  Zeilen-Summen-Kriterium erf\"ullt, dann konvergiert das Gesamtschritt-Verfahren f\"ur jeden
  Start-Vektor.  Bezeichnen wir die Vektoren der Iteration mit $\vec{x}^{(n)}$ und definieren wir
  \\[0.2cm]
  \hspace*{1.3cm}
  $\vec{x}^{(\infty)} := \lim\limits_{n\rightarrow\infty} \vec{x}^{(n)}$
  \\[0.2cm] 
  und ist weiter $q$ die Zahl aus dem starken Zeilen-Summen-Kriterium, dann gilt au{\ss}erdem die
  Absch\"atzung 
  \\[0.2cm]
  \hspace*{1.3cm}
  $\norm{\vec{x}^{(\infty)} - \vec{x}^{(n)}} \leq \frac{q^n}{1-q} \cdot \norm{\vec{x}^{(0)} - \vec{x}^{(1)}}$.
\end{Satz}

\proof
Wir definieren eine Funktion $f:\mathbb{R}^n \rightarrow \mathbb{R}^n$
komponentenweise:
\\[0.2cm]
\hspace*{1.3cm}
$f_i(\vec{x}) = \frac{1}{a_{ii}} \cdot  \Biggl( b_i - \sum\limits_{j=1 \atop j \not= i}^n a_{ij} \cdot  x_j \Biggr)$. 
\\[0.2cm]
Die $i$-te Komponente der Funktion $f$ berechnet also gerade die $i$-te
Komponente des Vektors $\vec{x}^{(n+1)}$.  Wir zeigen nun, dass f\"ur beliebige Vektoren
$\vec{x},\vec{y} \in \mathbb{R}^n$
\\[0.2cm]
\hspace*{1.3cm} 
$\norm{f(\vec{x}) - f(\vec{y}) } \leq q \cdot  \norm{\vec{x} - \vec{y} }$
\\[0.2cm]
gilt, denn dann ist $f$ eine kontrahierende Abbildung, und die Behauptung des
Satzes folgt aus dem Banach'schen Fixpunkt-Satz, den wir zwar nur f\"ur Funktionen 
$f: \mathbb{R} \rightarrow \mathbb{R}$ bewiesen haben, der aber auch f\"ur vektorwertige
Funktionen im $\mathbb{R}^n$ gilt, wenn wir den Betrag durch die Maximums-Norm ersetzen.
Wir sch\"atzen die Differenz $\bigl|f_i(\vec{x}) - f_i(\vec{y})\bigr|$ wie folgt ab:
\\[0.2cm]
\hspace*{0.5cm}
$
\begin{array}[t]{lclcl}
  \bigl|f_i(\vec{x}) - f_i(\vec{y})\bigr| & = & 
  \multicolumn{3}{l}{\biggl|\bruch{1}{a_{ii}} \cdot  \biggl( b_i - \sum\limits_{j=1 \atop j \not= i}^n a_{ij} \cdot  x_j \biggr) -
  \bruch{1}{a_{ii}} \cdot  \biggl( b_i - \sum\limits_{j=1 \atop j \not= i}^n a_{ij} \cdot  y_j \biggr) \biggr|}\\[0.7cm]
& = &
  \biggl|\bruch{1}{a_{ii}} \cdot  \sum\limits_{j=1 \atop j \not= i}^n a_{ij} \cdot  \bigl(y_j  - x_j \bigr)\biggr|
&\leq & \bruch{1}{|a_{ii}|} \cdot  \sum\limits_{j=1 \atop j \not= i}^n \bigl| a_{ij} \bigr| \cdot  \bigl|x_j  - y_j \bigr|\\[0.7cm]
&\leq & \bruch{1}{|a_{ii}|} \cdot  \sum\limits_{j=1 \atop j \not= i}^n \bigl| a_{ij} \bigr| \cdot  \norm{\vec{x} - \vec{y}} 
&\leq & \biggl(\bruch{1}{|a_{ii}|} \cdot  \sum\limits_{j=1 \atop j \not= i}^n \bigl| a_{ij} \bigr| \biggr)\cdot  \norm{\vec{x} - \vec{y}} \\[0.7cm]
&\leq & q \cdot  \norm{\vec{x} - \vec{y}} 
\end{array}
$
\\[0.3cm]
Da diese Ungleichung f\"ur alle $i=1,\cdots,n$ gilt, haben wir insgesamt die Absch\"atzung 
\\[0.2cm]
\hspace*{1.3cm}
$\norm{ f(\vec{x}) - f(\vec{y}) } \leq q \cdot  \norm{\vec{x} - \vec{y} }$
\\[0.2cm]
gezeigt und die Behauptung folgt nun aus dem Banach'schen Fixpunkt-Satz.
\hspace*{\fill} $\Box$
\vspace*{0.3cm}

Abbildung \ref{fig:jacobi-method.stlx} auf Seite \pageref{fig:jacobi-method.stlx} zeigt eine
einfache Implementierung des Jacobi-Verfahrens.   Die Funktion $\texttt{jakobi}()$ bekommt drei
Argumente:
\begin{enumerate}
\item $a$ ist eine $n \times n$ Matrix, die durch eine Liste dargestellt wird, deren
      Elemente selbst wieder Listen der L\"ange $n$ sind.  Auf das Matrix-Element
      $a_{ij}$ wird in \textsc{SetlX} dann durch den Ausdruck $a[i][j]$ zugegriffen. 
\item $b$ ist eine $n$-dimensionaler Vektor, der durch eine Liste der L\"ange $n$ dargestellt
      wird.
\item $k$ ist die Anzahl der durchzuf\"uhrenden Iterationen.
\end{enumerate}

\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 1.3cm,
                  xrightmargin  = 1.3cm,
                ]
    jacobi := procedure(a, b, k) {
        n := #b;
        assert(#a    == n, "wrong number of equations");
        assert(#a[1] == n, "wrong number of variables");
        x := xNew := [ 0 : i in [ 1 .. n ] ];
        for (l in [1 .. k]) {
            for (i in [1 .. n]) {
                xNew[i] := b[i];
                for (j in [ 1 .. n ]) {
                    if (i != j) {
                        xNew[i] -= a[i][j] * x[j];
                    }
                }
                xNew[i] /= a[i][i];
            }
            x := xNew;
            print("$l$: $x$");
        }
        return x;       
    };
    
    demo := procedure() {
        a := [ [ 4.0, 1.0, 0.0 ], 
               [ 1.0, 4.0, 1.0 ],
               [ 0.0, 1.0, 4.0 ] ];
        b := [ 5.0, 6.0, 5.0 ];
        k := 35;  
        x := jacobi(a, b, k);
        print("x = $x$");
    };
\end{Verbatim}
\vspace*{-0.3cm}
  \caption{Implementierung des Jacobi-Verfahrens.}
  \label{fig:jacobi-method.stlx}
\end{figure} %\$

\noindent
Die Funktion $\mathtt{jacobi}(a,b,x)$ versucht, mit Hilfe des Jacobi-Verfahrens das lineare
Gleichungs-System 
\\[0.2cm]
\hspace*{1.3cm}
$\textbf{a}\, \vec{x} = \vec{b}$
\\[0.2cm]
zu l\"osen und arbeitet im Detail wie folgt:
\begin{enumerate}
\item In Zeile 5 wid der Start-Vektor $\vec{x}^{(0)}$ als Null-Vektor definiert.
      Die Variable $\mathtt{xNew}$ speichert den Wert $\vec{x}^{(n+1)}$. 
\item Die Schleife, die in Zeile 6 beginnt, f\"uhrt insgesamt $k$ Iterationen des Jacobi-Verfahrens aus.
\item Die Schleife, die in Zeile 7 beginnt, berechnet den n\"achsten Wert $\vec{x}^{(n+1)}$
      gem\"a{\ss} der Formel
      \\[0.2cm]
      \hspace*{1.3cm}
      $x^{(n+1)}_i = 
       \bruch{1}{a_{ii}} \cdot  \Biggl( b_i - \sum\limits_{j=1 \atop j \not= i}^n a_{i,j} \cdot x_j \Biggr)
      $.   
      \\[0.2cm]
      Der Index $i$ l\"auft dabei \"uber die Komponenten des Vektors $\vec{x}^{(n+1)}$.
\end{enumerate}
Die Funktion \texttt{demo()} ruft die Funktion so auf, dass anschlie{\ss}end
das Gleichungs-System
\[
\left(\begin{array}[c]{lll}
  4.0 & 1.0 & 0.0 \\
  1.0 & 4.0 & 1.0 \\
  0.0 & 1.0 & 4.0 
\end{array}\right) \;\vec{x} =
\left(\begin{array}[c]{l}
  5.0 \\ 
  6.0 \\
  5.0
\end{array}\right)
\]
gel\"ost werden kann.  Die L\"osung dieses Systems ist
\\[0.2cm]
\hspace*{1.3cm}
 $\vec{x} =
\left(\begin{array}[c]{l}
  1.0 \\
  1.0 \\
  1.0
\end{array}\right)$.  
\\[0.2cm]
Diese L\"osung wird nach 35 Iterationen gefunden.
Tabelle \ref{tab:jacobi} auf Seite \pageref{tab:jacobi} zeigt den Verlauf der Rechnung.
Die Matrix $A$ erf\"ullt das starke Zeilen-Summen-Kriterium mit $q = \frac{1}{2}$.  Die exakte L\"osung wird
nach 35 Schritten gefunden.



\begin{table}[!h]
  \centering
\framebox{
  \begin{tabular}{|l|l|l|l|}
\hline
   $n$ & $x_1^{(n)}$ & ${x}_2^{(n)}$ &${x}_3^{(n)}$   \\
\hline
1 & 1.25 & 1.5 & 1.25 \\
\hline
2 & 0.875 & 0.875 & 0.875 \\
\hline
3 & 1.03125 & 1.0625 & 1.03125 \\
\hline
4 & 0.984375 & 0.984375 & 0.984375 \\
\hline
5 & 1.00390625 & 1.0078125 & 1.00390625 \\
\hline
6 & 0.998046875 & 0.998046875 & 0.998046875 \\
\hline
7 & 1.00048828125 & 1.0009765625 & 1.00048828125 \\
\hline
8 & 0.999755859375 & 0.999755859375 & 0.999755859375 \\
\hline
9 & 1.00006103515625 & 1.0001220703125 & 1.00006103515625 \\
\hline
10 & 0.999969482421875 & 0.999969482421875 & 0.999969482421875 \\
\hline
11 & 1.0000076293945312 & 1.0000152587890625 & 1.0000076293945312 \\
\hline
12 & 0.9999961853027344 & 0.9999961853027344 & 0.9999961853027344 \\
\hline
13 & 1.0000009536743164 & 1.0000019073486328 & 1.0000009536743164 \\
\hline
14 & 0.9999995231628418 & 0.9999995231628418 & 0.9999995231628418 \\
\hline
15 & 1.0000001192092896 & 1.000000238418579 & 1.0000001192092896 \\
\hline
16 & 0.9999999403953552 & 0.9999999403953552 & 0.9999999403953552 \\
\hline
17 & 1.0000000149011612 & 1.0000000298023224 & 1.0000000149011612 \\
\hline
18 & 0.9999999925494194 & 0.9999999925494194 & 0.9999999925494194 \\
\hline
19 & 1.0000000018626451 & 1.0000000037252903 & 1.0000000018626451 \\
\hline
20 & 0.9999999990686774 & 0.9999999990686774 & 0.9999999990686774 \\
\hline
21 & 1.0000000002328306 & 1.0000000004656613 & 1.0000000002328306 \\
\hline
22 & 0.9999999998835847 & 0.9999999998835847 & 0.9999999998835847 \\
\hline
23 & 1.0000000000291038 & 1.0000000000582077 & 1.0000000000291038 \\
\hline
24 & 0.9999999999854481 & 0.9999999999854481 & 0.9999999999854481 \\
\hline
25 & 1.000000000003638 & 1.000000000007276 & 1.000000000003638 \\
\hline
26 & 0.999999999998181 & 0.999999999998181 & 0.999999999998181 \\
\hline
27 & 1.0000000000004547 & 1.0000000000009095 & 1.0000000000004547 \\
\hline
28 & 0.9999999999997726 & 0.9999999999997726 & 0.9999999999997726 \\
\hline
29 & 1.0000000000000568 & 1.0000000000001137 & 1.0000000000000568 \\
\hline
30 & 0.9999999999999716 & 0.9999999999999716 & 0.9999999999999716 \\
\hline
31 & 1.000000000000007 & 1.0000000000000142 & 1.000000000000007 \\
\hline
32 & 0.9999999999999964 & 0.9999999999999964 & 0.9999999999999964 \\
\hline
33 & 1.0000000000000009 & 1.0000000000000018 & 1.0000000000000009 \\
\hline
34 & 0.9999999999999996 & 0.9999999999999996 & 0.9999999999999996 \\
\hline
35 & 1.0 & 1.0 & 1.0 \\
\hline
  \end{tabular}}
  \caption{Konvergenz des Jacobi-Verfahrens.}
  \label{tab:jacobi}
\end{table}




\subsubsection{Das Gau{\ss}-Seidel-Verfahren}
Betrachten wir die Implementierung des Gesamtschritt-Verfahrens, so liegt die folgende
Optimierung auf der Hand.  Wenn wir mit der Formel
\\[0.2cm]
\hspace*{1.3cm}
$x^{(n+1)}_i = \displaystyle \bruch{1}{a_{ii}} \cdot  \Biggl( b_i - \sum\limits_{j=1 \atop j \not= i}^n a_{ij} \cdot  x^{(n)}_j \Biggr)$
\\[0.2cm]
die $i$-te Komponente von $\vec{x}^{(n+1)}$ berechnen, dann kennen wir bereits die
neuen Komponenten $x^{(n+1)}_1,\cdots,\;x^{(n+1)}_{i-1}$. Da diese Komponenten (hoffentlich) n\"aher an der
L\"osung liegen als die Komponenten $x^{(n)}_1,\cdots,\;x^{(n)}_{i-1}$ des alten Vektors
$\vec{x}^{(n)}$, liegt es nahe, f\"ur $j<i$ die neuen Komponenten $x_j^{(n+1)}$ an Stelle
der alten Komponenten $x_j^{(n)}$ zu benutzen.  Damit kommen
wir zu der zun\"achst kompliziert aussehenden Iterations-Formel 
\\[0.2cm]
\hspace*{1.3cm}
$x^{(n+1)}_i = \displaystyle \bruch{1}{a_{ii}} \cdot  \Biggl( b_i - \sum\limits_{j=1}^{i-1} a_{ij} \cdot  x^{(n+1)}_j -
               \sum\limits_{j=i+1}^n a_{ij} \cdot  x^{(n)}_j \Biggr)$.
\\[0.2cm]
Das Verfahren, das auf dieser Formel basiert, wird als \emph{Einzelschritt-Verfahren} oder
auch 
\href{https://de.wikipedia.org/wiki/Einzelschrittverfahren}{\emph{Gau{\ss}-Seidel-Verfahren}} 
(\href{https://de.wikipedia.org/wiki/Philipp_Ludwig_von_Seidel}{Philipp Ludwig von Seidel}, 1821 -- 1896) bezeichnet.

Abbildung \ref{fig:seidel.stlx} zeigt, wie wir die Implementierung \"andern
m\"ussen, wenn wir das Gau{\ss}-Seidel-Verfahren anwenden wollen.  Die Implementierung des
Gau{\ss}-Seidel-Verfahrens ist einfacher als die Implementierung des Jacobi-Verfahrens, denn
wir ben\"otigen keine Hilfsvariable \texttt{xNew} mehr.
Tabelle \ref{tab:seidel} zeigt, dass das Gau{\ss}-Seidel-Verfahren f\"ur
unser Beispiel etwa doppelt so schnell konvergiert wie das Jacobi-Verfahren.
Es l\"asst sich zeigen, dass das Gau{\ss}-Seidel-Verfahren zur L\"osung immer dann konvergiert, wenn die Matrix
$\mathbf{A}$ des linearen Gleichungs-Systems
\\[0.2cm]
\hspace*{1.3cm}
$\mathbf{A} \vec{x} = \vec{b}$
\\[0.2cm]
das starke Zeilen-Summen-Kriterium erf\"ullt.  Allerdings ist die theoretische Analyse des
Gau{\ss}-Seidel-Verfahren schwieriger als die Untersuchung des Jacobi-Verfahrens, so dass wir von einer
detailierteren Diskussion aus Zeitgr\"unden absehen m\"ussen.

\begin{figure}[!ht]
  \centering
\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 1.3cm,
                  xrightmargin  = 1.3cm,
                ]
    gaussSeidel := procedure(a, b, k) {
        n := #b;
        assert(#a    == n, "wrong number of equations");
        assert(#a[1] == n, "wrong number of variables");
        x := [ 0 : i in [ 1 .. n ] ];
        for (l in [1 .. k]) {
            for (i in [1 .. n]) {
                x[i] := b[i];
                for (j in [ 1 .. n ]) {
                    if (i != j) {
                        x[i] -= a[i][j] * x[j];
                    }
                }
                x[i] /= a[i][i];
            }
            print("$l$: $x$");
        }
        return x;       
    };
    \end{Verbatim}
\vspace*{-0.3cm}
  \caption{Implementierung des Gau{\ss}-Seidel-Verfahrens.}
  \label{fig:seidel.stlx}
\end{figure} %\$


\begin{table}[!h]
  \centering
\framebox{
  \begin{tabular}{|l|l|l|l|}
\hline
   $n$ & $x_1^{(n)}$ & $x_2^{(n)}$ &$x_3^{(n)}$   \\
\hline
1 & 1.25 & 1.1875 & 0.953125 \\
\hline
2 & 0.953125 & 1.0234375 & 0.994140625 \\
\hline
3 & 0.994140625 & 1.0029296875 & 0.999267578125 \\
\hline
4 & 0.999267578125 & 1.0003662109375 & 0.999908447265625 \\
\hline
5 & 0.999908447265625 & 1.0000457763671875 & 0.9999885559082031 \\
\hline
6 & 0.9999885559082031 & 1.0000057220458984 & 0.9999985694885254 \\
\hline
7 & 0.9999985694885254 & 1.0000007152557373 & 0.9999998211860657 \\
\hline
8 & 0.9999998211860657 & 1.0000000894069672 & 0.9999999776482582 \\
\hline
9 & 0.9999999776482582 & 1.000000011175871 & 0.9999999972060323 \\
\hline
10 & 0.9999999972060323 & 1.0000000013969839 & 0.999999999650754 \\
\hline
11 & 0.999999999650754 & 1.000000000174623 & 0.9999999999563443 \\
\hline
12 & 0.9999999999563443 & 1.0000000000218279 & 0.999999999994543 \\
\hline
13 & 0.999999999994543 & 1.0000000000027285 & 0.9999999999993179 \\
\hline
14 & 0.9999999999993179 & 1.000000000000341 & 0.9999999999999147 \\
\hline
15 & 0.9999999999999147 & 1.0000000000000426 & 0.9999999999999893 \\
\hline
16 & 0.9999999999999893 & 1.0000000000000053 & 0.9999999999999987 \\
\hline
17 & 0.9999999999999987 & 1.0000000000000009 & 0.9999999999999998 \\
\hline
18 & 0.9999999999999998 & 1.0 & 1.0 \\
\hline
19 & 1.0 & 1.0 & 1.0 \\
\hline
  \end{tabular}}
  \caption{Konvergenz des Gau{\ss}-Seidel-Verfahrens.}
  \label{tab:seidel}
\end{table}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "analysis"
%%% End: 
